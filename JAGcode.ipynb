{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2d362-c99b-4903-af56-97c32512d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d35129-5778-41c2-8cdf-b59c3467c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import os\n",
    "import time\n",
    "import rasterio\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'DejaVu Sans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec048d97-16a4-4f64-8c23-e0c8fd07b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c84bc7-62b7-4b5b-a001-d311d26e9be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import zoom, gaussian_filter\n",
    "\n",
    "def upsample_bicubic_and_metrics(\n",
    "    lr_data, hr_data, scale=None,\n",
    "    save_tif_path=None, reference_tif_path=None\n",
    "):\n",
    "    \n",
    "    H_hr, W_hr = hr_data.shape\n",
    "    H_lr, W_lr = lr_data.shape\n",
    "    if scale is None:\n",
    "        zx, zy = H_hr / H_lr, W_hr / W_lr\n",
    "    else:\n",
    "        zx = zy = float(scale)\n",
    "\n",
    "    up = zoom(lr_data, zoom=(zx, zy), order=3, mode='reflect', prefilter=True)\n",
    "    up = up[:H_hr, :W_hr]\n",
    "    if up.shape != (H_hr, W_hr):  \n",
    "        pad_h, pad_w = H_hr - up.shape[0], W_hr - up.shape[1]\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            up = np.pad(up, ((0, max(0, pad_h)), (0, max(0, pad_w)))\n",
    "                        , mode='edge')\n",
    "\n",
    "    up = up.astype(np.float32, copy=False)\n",
    "    hr = hr_data.astype(np.float32, copy=False)\n",
    "\n",
    "    valid = np.isfinite(up) & np.isfinite(hr)\n",
    "    \n",
    "    diff = up - hr\n",
    "    mae = float(np.mean(np.abs(diff)[valid]))\n",
    "    rmse = float(np.sqrt(np.mean((diff[valid])**2)))\n",
    "\n",
    "    up_f = up.copy(); hr_f = hr.copy()\n",
    "    up_f[~valid] = np.nanmean(up[valid])\n",
    "    hr_f[~valid] = np.nanmean(hr[valid])\n",
    "\n",
    "    L = float(np.nanmax(hr_f) - np.nanmin(hr_f))\n",
    "    if not np.isfinite(L) or L == 0.0:\n",
    "        L = 1.0\n",
    "    K1, K2 = 0.01, 0.03\n",
    "    C1, C2 = (K1 * L) ** 2, (K2 * L) ** 2\n",
    "\n",
    "    mu_x = gaussian_filter(hr_f, sigma=1.5, truncate=3.5)\n",
    "    mu_y = gaussian_filter(up_f, sigma=1.5, truncate=3.5)\n",
    "    sigma_x2 = gaussian_filter(hr_f * hr_f, sigma=1.5, truncate=3.5) - mu_x * mu_x\n",
    "    sigma_y2 = gaussian_filter(up_f * up_f, sigma=1.5, truncate=3.5) - mu_y * mu_y\n",
    "    sigma_xy = gaussian_filter(hr_f * up_f, sigma=1.5, truncate=3.5) - mu_x * mu_y\n",
    "\n",
    "    ssim_map = ((2 * mu_x * mu_y + C1) * (2 * sigma_xy + C2)) / (\n",
    "               (mu_x**2 + mu_y**2 + C1) * (sigma_x2 + sigma_y2 + C2))\n",
    "    ssim = float(np.mean(ssim_map[valid]))\n",
    "    metrics = {'MAE': mae, 'RMSE': rmse, 'SSIM': ssim}\n",
    "\n",
    "    # ------------ 保存为 GeoTIFF（可选）------------\n",
    "    if save_tif_path is not None:\n",
    "        if reference_tif_path is None:\n",
    "            raise ValueError(\" reference_tif_path  CRS/transform。\")\n",
    "        import rasterio, os\n",
    "    \n",
    "        with rasterio.open(reference_tif_path) as ref:\n",
    "            profile = ref.profile\n",
    "        profile = profile.copy()\n",
    "        profile.update({\n",
    "            'height': up.shape[0],\n",
    "            'width':  up.shape[1],\n",
    "            'count':  1,\n",
    "            'dtype':  'float32',\n",
    "            'driver': 'GTiff',\n",
    "            'compress': 'lzw',   \n",
    "        })\n",
    "        dir_ = os.path.dirname(save_tif_path)\n",
    "        if dir_:\n",
    "            os.makedirs(dir_, exist_ok=True)\n",
    "        with rasterio.open(save_tif_path, 'w', **profile) as dst:\n",
    "            dst.write(up.astype(np.float32), 1)\n",
    "        print(f\"Upsampled (bicubic) GeoTIFF saved to: {save_tif_path}\")\n",
    "\n",
    "    return up, metrics\n",
    "\n",
    "\n",
    "\n",
    "up_bicubic, metrics = upsample_bicubic_and_metrics(\n",
    "    lr_data, hr_data, scale=4,\n",
    "    save_tif_path='JAG/upsampled_bicubic_4.tif',\n",
    "    reference_tif_path='JAG/磁异常.tif'  \n",
    ")\n",
    "print(\"Bicubic：\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91890e6d-e023-458a-b219-294db5babc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "lr_image_path = 'JAG/lr_map_simulated_4t.tif'  \n",
    "hr_image_path = 'JAG/磁异常.tif'            \n",
    "\n",
    "UPSAMPLE_INTERP_ORDER = 3\n",
    "\n",
    "UPSAMPLE_ANTIALIAS = True\n",
    "\n",
    "PATCH_SIZE = 64\n",
    "\n",
    "\n",
    "PATCH_STRIDE = 32\n",
    "\n",
    "NUM_PATCH_SHOW = 4 \n",
    "\n",
    "os.makedirs('JAG', exist_ok=True)\n",
    "\n",
    "def image_normalization(image_array: np.ndarray):\n",
    "    min_val = np.nanmin(image_array)\n",
    "    max_val = np.nanmax(image_array)\n",
    "    range_val = max_val - min_val\n",
    "    if range_val < 1e-6: \n",
    "        range_val = 1.0\n",
    "    normalized = (image_array - min_val) / range_val\n",
    "    normalized = np.nan_to_num(normalized, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    return normalized.astype(np.float32), float(min_val), float(max_val), float(range_val)\n",
    "\n",
    "def extract_patches(hr_img: np.ndarray, lr_img: np.ndarray, patch_size=64, stride=32):\n",
    "\n",
    "    assert hr_img.shape == lr_img.shape, \n",
    "    h, w = hr_img.shape\n",
    "    hr_patches, lr_patches = [], []\n",
    "\n",
    "    if h < patch_size or w < patch_size:\n",
    "        # 图像小于 patch，退化为单 patch\n",
    "        if h > 0 and w > 0:\n",
    "            print(f\"警告: 图像尺寸({h}x{w}) < patch 尺寸({patch_size}x{patch_size})，使用整图作为单个 patch\")\n",
    "            hr_patches.append(hr_img.copy())\n",
    "            lr_patches.append(lr_img.copy())\n",
    "            return hr_patches, lr_patches\n",
    "        else:\n",
    "            raise ValueError(f\"无效图像尺寸: {h}x{w}\")\n",
    "\n",
    "    for i in range(0, h - patch_size + 1, stride):\n",
    "        for j in range(0, w - patch_size + 1, stride):\n",
    "            hr_patch = hr_img[i:i+patch_size, j:j+patch_size]\n",
    "            lr_patch = lr_img[i:i+patch_size, j:j+patch_size]\n",
    "            # 逻辑上已保证 shape 正确；此处再次校验更稳妥\n",
    "            if hr_patch.shape == (patch_size, patch_size) and lr_patch.shape == (patch_size, patch_size):\n",
    "                hr_patches.append(hr_patch.copy())\n",
    "                lr_patches.append(lr_patch.copy())\n",
    "    return hr_patches, lr_patches\n",
    "\n",
    "def visualize_patch_pairs(lr_list, hr_list, num_show=4, save_path='results2/patch_examples.png'):\n",
    "\n",
    "    n = min(num_show, len(lr_list), len(hr_list))\n",
    "    if n <= 0:\n",
    "        print(\"警告: 没有可视化的 patch，跳过。\")\n",
    "        return\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(n):\n",
    "        plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(lr_list[i], cmap='gray')\n",
    "        plt.title(f'LR Patch {i+1}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(hr_list[i], cmap='gray')\n",
    "        plt.title(f'HR Patch {i+1}')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f'Patch 示例（上: LR / 下: HR）- 共 {len(hr_list)} 块')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    print(\"加载图像...\")\n",
    "    start_load = time.time()\n",
    "\n",
    "    with rasterio.open(hr_image_path) as src:\n",
    "        hr_array = src.read(1).astype(np.float32)\n",
    "        H_hr, W_hr = hr_array.shape\n",
    "        print(f\"高分辨率图像尺寸: {H_hr} x {W_hr}\")\n",
    "\n",
    "    with rasterio.open(lr_image_path) as src:\n",
    "        lr_array = src.read(1).astype(np.float32)\n",
    "        H_lr, W_lr = lr_array.shape\n",
    "        print(f\"低分辨率图像尺寸: {H_lr} x {W_lr}\")\n",
    "\n",
    "    print(\"上采样低分辨率图像以对齐 HR 尺寸...\")\n",
    "    lr_array_upsampled = resize(\n",
    "        lr_array,\n",
    "        (H_hr, W_hr),\n",
    "        order=UPSAMPLE_INTERP_ORDER,\n",
    "        anti_aliasing=UPSAMPLE_ANTIALIAS\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    print(\"归一化到 [0, 1]（以 HR min-max 为准）...\")\n",
    "    hr_array_norm, hr_min, hr_max, hr_range = image_normalization(hr_array)\n",
    "    lr_array_norm = (lr_array_upsampled - hr_min) / (hr_range if hr_range != 0 else 1.0)\n",
    "    lr_array_norm = np.clip(np.nan_to_num(lr_array_norm, nan=0.0, posinf=1.0, neginf=0.0), 0.0, 1.0).astype(np.float32)\n",
    "\n",
    "    Image.fromarray((hr_array_norm * 255).astype(np.uint8)).save('JAG/normalized_high_res.png')\n",
    "    Image.fromarray((lr_array_norm * 255).astype(np.uint8)).save('JAG/normalized_low_res.png')\n",
    "\n",
    "    print(f\"图像加载与预处理完成，耗时: {time.time() - start_load:.2f} 秒\")\n",
    "\n",
    "    print(\"提取图像块（patches）...\")\n",
    "    t0 = time.time()\n",
    "    hr_patches, lr_patches = extract_patches(\n",
    "        hr_array_norm, lr_array_norm,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        stride=PATCH_STRIDE\n",
    "    )\n",
    "\n",
    "    if len(hr_patches) < 4:\n",
    "        print(f\"警告: 仅提取到 {len(hr_patches)} 个 patch，尝试自动降低门槛...\")\n",
    "        new_patch_size = min(32, H_hr, W_hr)\n",
    "        new_stride = max(8, new_patch_size // 4)\n",
    "        print(f\"自动调整为：patch_size={new_patch_size}, stride={new_stride}\")\n",
    "        hr_patches, lr_patches = extract_patches(\n",
    "            hr_array_norm, lr_array_norm,\n",
    "            patch_size=new_patch_size,\n",
    "            stride=new_stride\n",
    "        )\n",
    "        print(f\"重新提取后共有 {len(hr_patches)} 个 patch\")\n",
    "\n",
    "    print(f\"最终提取 {len(hr_patches)} 个 patch，耗时: {time.time() - t0:.2f} 秒\")\n",
    "\n",
    "    visualize_patch_pairs(lr_patches, hr_patches, num_show=NUM_PATCH_SHOW, save_path='results2/patch_examples.png')\n",
    "\n",
    "    if len(hr_patches) == 0:\n",
    "        raise RuntimeError(\"未提取到任何 patch，请调小 PATCH_SIZE 或减小 STRIDE。\")\n",
    "\n",
    "    hr_patches_arr = np.stack(hr_patches, axis=0).astype(np.float32)\n",
    "    lr_patches_arr = np.stack(lr_patches, axis=0).astype(np.float32)\n",
    "\n",
    "    np.savez(\n",
    "        'JAG/preprocessed_data_4.npz',\n",
    "        hr_patches=hr_patches_arr,   # (N, H, W)\n",
    "        lr_patches=lr_patches_arr,   # (N, H, W)\n",
    "        hr_array_norm=hr_array_norm, # (H, W)\n",
    "        lr_array_norm=lr_array_norm, # (H, W)\n",
    "        hr_min=hr_min,\n",
    "        hr_max=hr_max,\n",
    "        hr_range=hr_range,\n",
    "        patch_size_used=int(hr_patches_arr.shape[1]),\n",
    "        patch_stride_used=int(PATCH_STRIDE),\n",
    "        upsample_order=int(UPSAMPLE_INTERP_ORDER)\n",
    "    )\n",
    "\n",
    "    print(\"\\n=========== 预处理完成 Summary ===========\")\n",
    "    print(f\"HR 尺寸: {H_hr} x {W_hr}\")\n",
    "    print(f\"LR 原始尺寸: {H_lr} x {W_lr}  -> 上采样到 HR 尺寸, order={UPSAMPLE_INTERP_ORDER}\")\n",
    "    print(f\"归一化参数: min={hr_min:.3f}, max={hr_max:.3f}, range={hr_range:.3f}\")\n",
    "    print(f\"Patch: size={hr_patches_arr.shape[1]} stride={PATCH_STRIDE}  -> 数量={len(hr_patches_arr)}\")\n",
    "    print(f\"可视化: results/patch_examples.png\")\n",
    "    print(\"数据包: JAG/preprocessed_data_4.npz\")\n",
    "    print(\"=========================================\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb90f6-d9d9-44cd-9647-5bf6cde9f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio, numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "hr_path = 'JAG/磁异常.tif'\n",
    "lr_path = 'JAG/lr_map_simulated_4t.tif'\n",
    "scale   = 4\n",
    "\n",
    "with rasterio.open(hr_path) as hr_ds, rasterio.open(lr_path) as lr_ds:\n",
    "    \n",
    "    hr_arr = hr_ds.read(1).astype(np.float32)\n",
    "    lr_arr = lr_ds.read(1).astype(np.float32)\n",
    "\n",
    "    Hh, Wh = hr_ds.height, hr_ds.width\n",
    "    Hl, Wl = lr_ds.height, lr_ds.width\n",
    "    \n",
    "    ratio_x = abs(lr_ds.transform.a / hr_ds.transform.a)\n",
    "    ratio_y = abs(lr_ds.transform.e / hr_ds.transform.e)\n",
    "    print(f\"HR({Hh}x{Wh})  LR({Hl}x{Wl})  pixel-size ratio ≈ {ratio_x:.3f}, {ratio_y:.3f}\")\n",
    "\n",
    "   \n",
    "    H2, W2 = (Hh // scale) * scale, (Wh // scale) * scale\n",
    "    if (H2, W2) != (Hh, Wh):\n",
    "        hr_arr = hr_arr[:H2, :W2]\n",
    "        print(f\"Cropped HR to ({H2},{W2}) so {scale}× divides both dims.\")\n",
    "\n",
    "    \n",
    "    lr_up = resize(\n",
    "        lr_arr, hr_arr.shape,\n",
    "        order=3, anti_aliasing=False, preserve_range=True\n",
    "    ).astype(np.float32)\n",
    "\n",
    "\n",
    "mn, mx = float(np.nanmin(hr_arr)), float(np.nanmax(hr_arr))\n",
    "rg = mx - mn if mx > mn else 1.0\n",
    "hr01 = np.clip((hr_arr - mn) / rg, 0, 1).astype(np.float32)\n",
    "lr01 = np.clip((lr_up - mn) / rg, 0, 1).astype(np.float32)\n",
    "\n",
    "print('OK for 4× SR' if hr01.shape == lr01.shape else 'mismatch!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e370782-410c-4306-9106-5438b90d2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029705fc-0e1d-4063-a0a5-ac1a0d75e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############SRCNN#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d200d-aaaa-4eb6-bcae-2aaf06d63a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "NUM_EPOCHS      = 1000\n",
    "INIT_LR         = 1e-4\n",
    "BATCH_SIZE      = 64                 \n",
    "EVAL_EVERY      = 1                  \n",
    "VAL_RATIO       = 0.1                \n",
    "VAL_MAX_SAMPLES = 512                \n",
    "DS_FACTOR       = 4                  \n",
    "EARLY_STOP      = 10                 \n",
    "USE_COMPILE     = True               \n",
    "MODEL_PATH      = 'JAG/best_srcnn_fast.pth'\n",
    "\n",
    "\n",
    "hr_image_path   = 'JAG/磁异常.tif'\n",
    "\n",
    "\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "\n",
    "\n",
    "data = np.load('JAG/preprocessed_data_3.npz', allow_pickle=True)\n",
    "hr_patches    = data['hr_patches']         # (N, H, W), 0~1\n",
    "lr_patches    = data['lr_patches']         # (N, H, W), 0~1\n",
    "hr_array_norm = data['hr_array_norm']      # (H, W),   0~1\n",
    "lr_array_norm = data['lr_array_norm']      # (H, W),   0~1\n",
    "hr_min  = float(data['hr_min'])\n",
    "hr_max  = float(data['hr_max'])\n",
    "hr_range = float(data['hr_range'])\n",
    "\n",
    "\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=9, padding=4)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(32, 1, kernel_size=5, padding=2)\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        return self.conv3(x)\n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, hr_patches, lr_patches):\n",
    "        self.hr = np.ascontiguousarray(hr_patches.astype(np.float32))\n",
    "        self.lr = np.ascontiguousarray(lr_patches.astype(np.float32))\n",
    "    def __len__(self): return self.hr.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        lr = torch.from_numpy(self.lr[idx]).unsqueeze(0)  # (1,H,W)\n",
    "        hr = torch.from_numpy(self.hr[idx]).unsqueeze(0)\n",
    "        return lr, hr\n",
    "\n",
    "N = len(hr_patches)\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "split = int(N * (1 - VAL_RATIO))\n",
    "train_idx, val_idx = idx[:split], idx[split:]\n",
    "train_set = SuperResolutionDataset(hr_patches[train_idx], lr_patches[train_idx])\n",
    "val_set   = SuperResolutionDataset(hr_patches[val_idx],  lr_patches[val_idx])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_workers = min(8, os.cpu_count() or 4)\n",
    "pin = torch.cuda.is_available()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin,\n",
    "    persistent_workers=(num_workers > 0),\n",
    "    prefetch_factor=4,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=min(BATCH_SIZE, 32),\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin,\n",
    "    persistent_workers=(num_workers > 0),\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "\n",
    "use_torchmetrics = True\n",
    "try:\n",
    "    from torchmetrics.functional import structural_similarity_index_measure as tm_ssim\n",
    "    from torchmetrics.functional import peak_signal_noise_ratio as tm_psnr\n",
    "except Exception:\n",
    "    use_torchmetrics = False\n",
    "    from skimage.metrics import structural_similarity as sk_ssim\n",
    "\n",
    "def _downsample_torch(x, factor=1):\n",
    "    if factor <= 1: return x\n",
    "    return torch.nn.functional.avg_pool2d(x, kernel_size=factor, stride=factor, ceil_mode=False)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_on_val(model, loader, eval_max=512, ds_factor=2, device='cuda'):\n",
    "    \"\"\"在验证集 patch 上评估（稀疏+可下采样），返回 PSNR/SSIM/MAE/RMSE（0~1 空间）。\"\"\"\n",
    "    model.eval()\n",
    "    n_seen = 0\n",
    "    psnr_list, ssim_list, mae_list, rmse_list = [], [], [], []\n",
    "\n",
    "    for lr, hr in loader:\n",
    "        if n_seen >= eval_max:\n",
    "            break\n",
    "        lr = lr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        hr = hr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            sr = model(lr)\n",
    "\n",
    "       \n",
    "        H = min(sr.shape[-2], hr.shape[-2])\n",
    "        W = min(sr.shape[-1], hr.shape[-1])\n",
    "        sr = sr[..., :H, :W].clamp_(0, 1)\n",
    "        hr = hr[..., :H, :W].clamp_(0, 1)\n",
    "\n",
    "       \n",
    "        sr_eval = _downsample_torch(sr, ds_factor)\n",
    "        hr_eval = _downsample_torch(hr, ds_factor)\n",
    "\n",
    "        if use_torchmetrics:\n",
    "            psnr = tm_psnr(sr_eval, hr_eval, data_range=1.0)\n",
    "            ssim = tm_ssim(sr_eval, hr_eval, data_range=1.0)\n",
    "            mae  = torch.mean(torch.abs(sr_eval - hr_eval))\n",
    "            mse  = torch.mean((sr_eval - hr_eval) ** 2)\n",
    "            rmse = torch.sqrt(mse)\n",
    "\n",
    "            psnr_list.append(psnr.detach().float().item())\n",
    "            ssim_list.append(ssim.detach().float().item())\n",
    "            mae_list.append(mae.detach().float().item())\n",
    "            rmse_list.append(rmse.detach().float().item())\n",
    "        else:\n",
    "            \n",
    "            sr_cpu = sr_eval.squeeze(1).detach().cpu().numpy()  # (B,H,W)\n",
    "            hr_cpu = hr_eval.squeeze(1).detach().cpu().numpy()\n",
    "            for i in range(sr_cpu.shape[0]):\n",
    "                mse  = np.mean((hr_cpu[i] - sr_cpu[i])**2)\n",
    "                rmse = float(np.sqrt(mse))\n",
    "                psnr = 20 * np.log10(1.0 / (np.sqrt(mse) + 1e-12))\n",
    "                ssim = sk_ssim(hr_cpu[i], sr_cpu[i], data_range=1.0)\n",
    "                mae  = float(np.mean(np.abs(hr_cpu[i] - sr_cpu[i])))\n",
    "\n",
    "                psnr_list.append(float(psnr))\n",
    "                ssim_list.append(float(ssim))\n",
    "                mae_list.append(float(mae))\n",
    "                rmse_list.append(float(rmse))\n",
    "\n",
    "        n_seen += lr.size(0)\n",
    "\n",
    "    if len(psnr_list) == 0:\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    return (\n",
    "        float(np.mean(psnr_list)),\n",
    "        float(np.mean(ssim_list)),\n",
    "        float(np.mean(mae_list)),\n",
    "        float(np.mean(rmse_list)),\n",
    "    )\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = SRCNN().to(device).to(memory_format=torch.channels_last)\n",
    "if USE_COMPILE:\n",
    "    try:\n",
    "        model = torch.compile(model, mode=\"reduce-overhead\")  # 或 \"max-autotune\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=INIT_LR, fused=torch.cuda.is_available())\n",
    "criterion = nn.L1Loss()\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',        \n",
    "    factor=0.5,        \n",
    "    patience=10,       \n",
    "    verbose=True,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "best_score = -float('inf')  \n",
    "epochs_no_improve = 0\n",
    "\n",
    "def _get_lr(optim_):\n",
    "    for pg in optim_.param_groups:\n",
    "        return pg.get(\"lr\", None)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    running = 0.0\n",
    "\n",
    "    for lr, hr in train_loader:\n",
    "        lr = lr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        hr = hr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            pred = model(lr)\n",
    "            loss = criterion(pred, hr)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running += loss.item()\n",
    "\n",
    "    avg_loss = running / len(train_loader)\n",
    "    log = f\"[Epoch {epoch}/{NUM_EPOCHS}] lr={_get_lr(optimizer):.2e} loss={avg_loss:.6f} time={time.time()-t0:.1f}s\"\n",
    "\n",
    "    if epoch % EVAL_EVERY == 0:\n",
    "        psnr, ssim, mae, rmse = evaluate_on_val(\n",
    "            model, val_loader,\n",
    "            eval_max=VAL_MAX_SAMPLES,\n",
    "            ds_factor=DS_FACTOR,\n",
    "            device=device\n",
    "        )\n",
    "  \n",
    "        log += f\" | val_mae={mae:.4f} val_rmse={rmse:.4f} val_ssim={ssim:.4f} score={score:.4f}\"\n",
    "        scheduler.step(score)\n",
    "        if score > best_score + 1e-6:\n",
    "            best_score = score\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= EARLY_STOP:\n",
    "            print(log)\n",
    "            print(\">>> 早停触发（组合分数连续无改善）\")\n",
    "            break\n",
    "\n",
    "    print(log)\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_full(model, lr_image_01, device='cuda'):\n",
    "    model.eval()\n",
    "    x = torch.from_numpy(lr_image_01.astype(np.float32)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "        y = model(x).squeeze(0).squeeze(0).clamp_(0, 1)\n",
    "    return y.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "best = SRCNN().to(device)\n",
    "if USE_COMPILE:\n",
    "    try:\n",
    "        best = torch.compile(best, mode=\"reduce-overhead\")\n",
    "    except Exception:\n",
    "        pass\n",
    "best.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "best.eval()\n",
    "\n",
    "sr_01 = predict_full(best, lr_array_norm, device=device)\n",
    "\n",
    "\n",
    "sr_orig = (sr_01 * hr_range) + hr_min\n",
    "sr_orig = np.clip(sr_orig, hr_min, hr_max).astype(np.float32)\n",
    "hr_orig = (hr_array_norm * hr_range) + hr_min\n",
    "\n",
    "hr_clean = np.nan_to_num(hr_orig, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float64)\n",
    "sr_clean = np.nan_to_num(sr_orig, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float64)\n",
    "\n",
    "finite_mask = np.isfinite(hr_clean) & np.isfinite(sr_clean)\n",
    "if finite_mask.sum() == 0:\n",
    "    raise RuntimeError(\"整图没有可用的像素\")\n",
    "\n",
    "hr_f = hr_clean[finite_mask]\n",
    "sr_f = sr_clean[finite_mask]\n",
    "\n",
    "hr_min_f = float(np.min(hr_f))\n",
    "hr_max_f = float(np.max(hr_f))\n",
    "data_range_f = hr_max_f - hr_min_f\n",
    "\n",
    "diff = sr_f - hr_f\n",
    "mse_val = float(np.mean(diff * diff))\n",
    "mae_val = float(np.mean(np.abs(diff)))\n",
    "rmse_val = float(np.sqrt(mse_val))\n",
    "\n",
    "eps = 1e-12\n",
    "if data_range_f < eps:\n",
    "    psnr_val = float('inf') if mse_val < eps else 20 * np.log10((1.0) / np.sqrt(mse_val + eps))\n",
    "else:\n",
    "    psnr_val = 20 * np.log10(data_range_f / np.sqrt(mse_val + eps))\n",
    "\n",
    "if data_range_f < eps:\n",
    "    ssim_val = 1.0 if mse_val < eps else 0.0\n",
    "else:\n",
    "    hr01 = (hr_f - hr_min_f) / (data_range_f + eps)\n",
    "    sr01 = (sr_f - hr_min_f) / (data_range_f + eps)\n",
    "    if use_torchmetrics and torch.cuda.is_available():\n",
    "        hr_t = torch.from_numpy(hr01.reshape(1, 1, -1, 1)).to(device)  # 形状不重要，只要是 2D\n",
    "        sr_t = torch.from_numpy(sr01.reshape(1, 1, -1, 1)).to(device)\n",
    "        with torch.amp.autocast('cuda', enabled=True):\n",
    "            ssim_val = float(tm_ssim(sr_t, hr_t, data_range=1.0).item())\n",
    "    else:\n",
    "        from skimage.metrics import structural_similarity as sk_ssim\n",
    "        H = int(np.sqrt(hr01.size)) or 1\n",
    "        W = int(np.ceil(hr01.size / H))\n",
    "        pad = H * W - hr01.size\n",
    "        if pad > 0:\n",
    "            hr01 = np.pad(hr01, (0, pad), constant_values=0)\n",
    "            sr01 = np.pad(sr01, (0, pad), constant_values=0)\n",
    "        hr_img = hr01.reshape(H, W)\n",
    "        sr_img = sr01.reshape(H, W)\n",
    "        ssim_val = float(sk_ssim(hr_img, sr_img, data_range=1.0))\n",
    "\n",
    "print(\"\\n===== Final Model Metrics (Full Image) =====\")\n",
    "print(f\"PSNR: {psnr_val:.4f} dB\")\n",
    "print(f\"SSIM: {ssim_val:.4f}\")\n",
    "print(f\"MAE : {mae_val:.6f}\")\n",
    "print(f\"MSE : {mse_val:.6f}\")\n",
    "print(f\"RMSE: {rmse_val:.6f}\")\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "import rasterio\n",
    "with rasterio.open(hr_image_path) as src:\n",
    "    profile = src.profile\n",
    "profile.update(count=1, dtype='float32')\n",
    "\n",
    "out_tif = 'JAG/srcnn_reconstructed_3.tif'\n",
    "with rasterio.open(out_tif, 'w', **profile) as dst:\n",
    "    dst.write(sr_orig, 1)\n",
    "\n",
    "print(f\">>> 推理完成，GeoTIFF 保存：{out_tif}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab56feda-bbfd-4bb8-85d7-67cb855cf32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4746f00e-6339-48e1-811b-1234f9c7f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "NUM_EPOCHS      = 600       \n",
    "INIT_LR         = 1e-4      \n",
    "#INIT_LR         = 5e-5      \n",
    "BATCH_SIZE      = 128       \n",
    "EVAL_EVERY      = 1\n",
    "#VAL_RATIO       = 0.2      \n",
    "VAL_RATIO       = 0.3       \n",
    "VAL_MAX_SAMPLES = 4096      \n",
    "DS_FACTOR       = 1         \n",
    "EARLY_STOP      = 5        \n",
    "USE_COMPILE     = True\n",
    "MODEL_PATH      = 'JAG/best_rdn_fast.pth'\n",
    "\n",
    "W_SSIM = 0.7\n",
    "W_MAE  = 0.3\n",
    "\n",
    "SCALE_IN_MODEL  = 1 \n",
    "\n",
    "\n",
    "hr_image_path   = 'JAG/磁异常.tif'\n",
    "\n",
    "\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "\n",
    "\n",
    "data = np.load('JAG/preprocessed_data_4.npz', allow_pickle=True)\n",
    "hr_patches    = data['hr_patches']         # (N, H, W), 0~1\n",
    "lr_patches    = data['lr_patches']         # (N, H, W), 0~1\n",
    "hr_array_norm = data['hr_array_norm']      # (H, W),   0~1\n",
    "lr_array_norm = data['lr_array_norm']      # (H, W),   0~1\n",
    "hr_min  = float(data['hr_min'])\n",
    "hr_max  = float(data['hr_max'])\n",
    "hr_range = float(data['hr_range'])\n",
    "\n",
    "\n",
    "class ResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, nf=64, gc=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(nf + 2*gc, gc, 3, 1, 1)\n",
    "        self.conv4 = nn.Conv2d(nf + 3*gc, gc, 3, 1, 1)\n",
    "        self.conv5 = nn.Conv2d(nf + 4*gc, nf, 3, 1, 1)\n",
    "        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "    def forward(self, x):\n",
    "        x1 = self.lrelu(self.conv1(x))\n",
    "        x2 = self.lrelu(self.conv2(torch.cat([x, x1], 1)))\n",
    "        x3 = self.lrelu(self.conv3(torch.cat([x, x1, x2], 1)))\n",
    "        x4 = self.lrelu(self.conv4(torch.cat([x, x1, x2, x3], 1)))\n",
    "        x5 = self.conv5(torch.cat([x, x1, x2, x3, x4], 1))\n",
    "        return x + 0.2 * x5\n",
    "\n",
    "class ResidualDenseNetwork(nn.Module):\n",
    "    def __init__(self, in_nc=1, out_nc=1, nf=64, nb=16, gc=32, scale=1):\n",
    "        super().__init__()\n",
    "        assert scale in (1, 2, 4, 8)\n",
    "        self.scale = scale\n",
    "        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1)\n",
    "        self.RDBs = nn.ModuleList([ResidualDenseBlock(nf, gc) for _ in range(nb)])\n",
    "        self.gff = nn.Sequential(\n",
    "            nn.Conv2d(nb * nf, nf, 1, 1, 0),\n",
    "            nn.Conv2d(nf, nf, 3, 1, 1)\n",
    "        )\n",
    "        self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1)\n",
    "        self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1)\n",
    "        self.upconv3 = nn.Conv2d(nf, nf, 3, 1, 1)\n",
    "        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1)\n",
    "        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fea = self.conv_first(x)\n",
    "        fea_shallow = fea.clone()\n",
    "        rdb_outs = []\n",
    "        for rdb in self.RDBs:\n",
    "            fea = rdb(fea)\n",
    "            rdb_outs.append(fea)\n",
    "        fea_long = torch.cat(rdb_outs, 1)\n",
    "        fea_gff = self.gff(fea_long)\n",
    "        fea = fea_shallow + fea_gff\n",
    "        if self.scale == 1:\n",
    "            pass\n",
    "        elif self.scale == 2:\n",
    "            fea = self.lrelu(self.upconv1(F.interpolate(fea, scale_factor=2, mode='nearest')))\n",
    "        elif self.scale == 4:\n",
    "            fea = self.lrelu(self.upconv1(F.interpolate(fea, scale_factor=2, mode='nearest')))\n",
    "            fea = self.lrelu(self.upconv2(F.interpolate(fea, scale_factor=2, mode='nearest')))\n",
    "        elif self.scale == 8:\n",
    "            fea = self.lrelu(self.upconv1(F.interpolate(fea, scale_factor=2, mode='nearest')))\n",
    "            fea = self.lrelu(self.upconv2(F.interpolate(fea, scale_factor=2, mode='nearest')))\n",
    "            fea = self.lrelu(self.upconv3(F.interpolate(fea, scale_factor=2, mode='nearest')))\n",
    "        out = self.conv_last(fea)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, hr_patches, lr_patches):\n",
    "        self.hr = np.ascontiguousarray(hr_patches.astype(np.float32))\n",
    "        self.lr = np.ascontiguousarray(lr_patches.astype(np.float32))\n",
    "    def __len__(self): return self.hr.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        lr = torch.from_numpy(self.lr[idx]).unsqueeze(0)\n",
    "        hr = torch.from_numpy(self.hr[idx]).unsqueeze(0)\n",
    "        return lr, hr\n",
    "\n",
    "N = len(hr_patches)\n",
    "idx = np.arange(N); np.random.shuffle(idx)\n",
    "\n",
    "if N == 0:\n",
    "    raise RuntimeError(\"数据集中没有任何 patch\")\n",
    "\n",
    "# 先按 VAL_RATIO 计算，再裁剪到 [1, N-1]\n",
    "proposed_split = int(N * (1 - VAL_RATIO))\n",
    "if proposed_split <= 0:\n",
    "    split = 1 if N > 1 else 1  # N==1 也给 1，后续会让 val=同一条\n",
    "elif proposed_split >= N:\n",
    "    split = N - 1\n",
    "else:\n",
    "    split = proposed_split\n",
    "\n",
    "train_idx = idx[:split]\n",
    "val_idx   = idx[split:]\n",
    "\n",
    "if N == 1:\n",
    "    train_idx = idx\n",
    "    val_idx   = idx\n",
    "    print(\">>> 警告：仅有 1 个 patch。\")\n",
    "\n",
    "train_set = SuperResolutionDataset(hr_patches[train_idx], lr_patches[train_idx])\n",
    "val_set   = SuperResolutionDataset(hr_patches[val_idx],  lr_patches[val_idx])\n",
    "\n",
    "n_train, n_val = len(train_set), len(val_set)\n",
    "print(f\">>> 数据切分：N={N}, train={n_train}, val={n_val}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_workers = min(8, os.cpu_count() or 4)\n",
    "pin = torch.cuda.is_available()\n",
    "\n",
    "TRAIN_BS = max(1, min(BATCH_SIZE, n_train))\n",
    "VAL_BS   = max(1, min(BATCH_SIZE, n_val, 32))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=TRAIN_BS, shuffle=True,\n",
    "    num_workers=num_workers, pin_memory=pin,\n",
    "    persistent_workers=(num_workers>0), prefetch_factor=4, drop_last=False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, batch_size=VAL_BS, shuffle=False,\n",
    "    num_workers=num_workers, pin_memory=pin,\n",
    "    persistent_workers=(num_workers>0), prefetch_factor=4\n",
    ")\n",
    "\n",
    "use_torchmetrics = True\n",
    "try:\n",
    "    from torchmetrics.functional import structural_similarity_index_measure as tm_ssim\n",
    "    from torchmetrics.functional import peak_signal_noise_ratio as tm_psnr\n",
    "except Exception:\n",
    "    use_torchmetrics = False\n",
    "    from skimage.metrics import structural_similarity as sk_ssim\n",
    "\n",
    "def _downsample_torch(x, factor=1):\n",
    "    if factor <= 1: return x\n",
    "    return torch.nn.functional.avg_pool2d(x, kernel_size=factor, stride=factor, ceil_mode=False)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_on_val(model, loader, eval_max=512, ds_factor=2, device='cuda'):\n",
    "    model.eval()\n",
    "    if len(loader) == 0:\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "    n_seen = 0\n",
    "    psnr_list, ssim_list, mae_list, rmse_list = [], [], [], []\n",
    "    for lr, hr in loader:\n",
    "        if n_seen >= eval_max: break\n",
    "        lr = lr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        hr = hr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            sr = model(lr)\n",
    "        H = min(sr.shape[-2], hr.shape[-2]); W = min(sr.shape[-1], hr.shape[-1])\n",
    "        sr = sr[..., :H, :W].clamp_(0, 1); hr = hr[..., :H, :W].clamp_(0, 1)\n",
    "        sr_eval = _downsample_torch(sr, ds_factor)\n",
    "        hr_eval = _downsample_torch(hr, ds_factor)\n",
    "        if use_torchmetrics:\n",
    "            psnr = tm_psnr(sr_eval, hr_eval, data_range=1.0)\n",
    "            ssim = tm_ssim(sr_eval, hr_eval, data_range=1.0)\n",
    "            mae  = torch.mean(torch.abs(sr_eval - hr_eval))\n",
    "            mse  = torch.mean((sr_eval - hr_eval) ** 2)\n",
    "            rmse = torch.sqrt(mse)\n",
    "            psnr_list.append(psnr.detach().float().item())\n",
    "            ssim_list.append(ssim.detach().float().item())\n",
    "            mae_list.append(mae.detach().float().item())\n",
    "            rmse_list.append(rmse.detach().float().item())\n",
    "        else:\n",
    "            sr_cpu = sr_eval.squeeze(1).detach().cpu().numpy()\n",
    "            hr_cpu = hr_eval.squeeze(1).detach().cpu().numpy()\n",
    "            for i in range(sr_cpu.shape[0]):\n",
    "                diff = hr_cpu[i] - sr_cpu[i]\n",
    "                mse  = float(np.mean(diff * diff))\n",
    "                rmse = float(np.sqrt(mse))\n",
    "                psnr = 20 * np.log10(1.0 / (np.sqrt(mse) + 1e-12))\n",
    "                ssim = sk_ssim(hr_cpu[i], sr_cpu[i], data_range=1.0)\n",
    "                mae  = float(np.mean(np.abs(diff)))\n",
    "                psnr_list.append(psnr); ssim_list.append(ssim); mae_list.append(mae); rmse_list.append(rmse)\n",
    "        n_seen += lr.size(0)\n",
    "    if len(psnr_list) == 0: return 0.0, 0.0, 0.0, 0.0\n",
    "    return (float(np.mean(psnr_list)), float(np.mean(ssim_list)),\n",
    "            float(np.mean(mae_list)),  float(np.mean(rmse_list)))\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = ResidualDenseNetwork(in_nc=1, out_nc=1, nf=64, nb=16, gc=32, scale=SCALE_IN_MODEL).to(device).to(memory_format=torch.channels_last)\n",
    "if USE_COMPILE:\n",
    "    try: model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "    except Exception: pass\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=INIT_LR,\n",
    "    betas=(0.9, 0.98),\n",
    "    weight_decay=1e-3,\n",
    "    fused=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n",
    ")\n",
    "\n",
    "best_score = -float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "def _get_lr(optim_):\n",
    "    for pg in optim_.param_groups: return pg.get(\"lr\", None)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train(); t0 = time.time(); running = 0.0\n",
    "    num_batches = 0\n",
    "    for lr, hr in train_loader:\n",
    "        lr = lr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        hr = hr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            pred = model(lr); loss = criterion(pred, hr)\n",
    "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "        running += loss.item(); num_batches += 1\n",
    "    avg_loss = running / max(1, num_batches)\n",
    "    log = f\"[Epoch {epoch}/{NUM_EPOCHS}] lr={_get_lr(optimizer):.2e} loss={avg_loss:.6f} time={time.time()-t0:.1f}s\"\n",
    "\n",
    "    if epoch % EVAL_EVERY == 0:\n",
    "        psnr, ssim, mae, rmse = evaluate_on_val(model, val_loader, eval_max=VAL_MAX_SAMPLES, ds_factor=DS_FACTOR, device=device)\n",
    "        score = W_SSIM * ssim + W_MAE * (1.0 - mae)\n",
    "        log += f\" | val_mae={mae:.4f} val_rmse={rmse:.4f} val_ssim={ssim:.4f} score={score:.4f}\"\n",
    "        scheduler.step(score)\n",
    "        if score > best_score + 1e-6:\n",
    "            best_score = score; epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= EARLY_STOP:\n",
    "            print(log); print(\">>> 早停触发\"); break\n",
    "    print(log)\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_full(model, lr_image_01, device='cuda'):\n",
    "    model.eval()\n",
    "    x = torch.from_numpy(lr_image_01.astype(np.float32)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "        y = model(x).squeeze(0).squeeze(0).clamp_(0, 1)\n",
    "    return y.detach().cpu().numpy()\n",
    "\n",
    "best = ResidualDenseNetwork(in_nc=1, out_nc=1, nf=64, nb=16, gc=32, scale=SCALE_IN_MODEL).to(device)\n",
    "if USE_COMPILE:\n",
    "    try: best = torch.compile(best, mode=\"reduce-overhead\")\n",
    "    except Exception: pass\n",
    "best.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "best.eval()\n",
    "\n",
    "sr_01 = predict_full(best, lr_array_norm, device=device)\n",
    "\n",
    "\n",
    "sr_orig = (sr_01 * hr_range) + hr_min\n",
    "sr_orig = np.clip(sr_orig, hr_min, hr_max).astype(np.float32)\n",
    "hr_orig = (hr_array_norm * hr_range) + hr_min\n",
    "\n",
    "\n",
    "hr_clean = np.nan_to_num(hr_orig, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float64)\n",
    "sr_clean = np.nan_to_num(sr_orig, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float64)\n",
    "finite_mask = np.isfinite(hr_clean) & np.isfinite(sr_clean)\n",
    "if finite_mask.sum() == 0: raise RuntimeError(\"整图没有可用的有限像素用于评估。\")\n",
    "hr_f = hr_clean[finite_mask]; sr_f = sr_clean[finite_mask]\n",
    "hr_min_f = float(np.min(hr_f)); hr_max_f = float(np.max(hr_f))\n",
    "data_range_f = hr_max_f - hr_min_f\n",
    "diff = sr_f - hr_f\n",
    "mse_val = float(np.mean(diff * diff))\n",
    "mae_val = float(np.mean(np.abs(diff)))\n",
    "rmse_val = float(np.sqrt(mse_val))\n",
    "eps = 1e-12\n",
    "if data_range_f < eps:\n",
    "    psnr_val = float('inf') if mse_val < eps else 20 * np.log10(1.0 / np.sqrt(mse_val + eps))\n",
    "else:\n",
    "    psnr_val = 20 * np.log10(data_range_f / np.sqrt(mse_val + eps))\n",
    "\n",
    "if data_range_f < eps:\n",
    "    ssim_val = 1.0 if mse_val < eps else 0.0\n",
    "else:\n",
    "    if use_torchmetrics and torch.cuda.is_available():\n",
    "        hr01 = (hr_f - hr_min_f)/(data_range_f + eps)\n",
    "        sr01 = (sr_f - hr_min_f)/(data_range_f + eps)\n",
    "        hr_t = torch.from_numpy(hr01.reshape(1,1,-1,1)).to(device)\n",
    "        sr_t = torch.from_numpy(sr01.reshape(1,1,-1,1)).to(device)\n",
    "        with torch.amp.autocast('cuda', enabled=True):\n",
    "            ssim_val = float(tm_ssim(sr_t, hr_t, data_range=1.0).item())\n",
    "    else:\n",
    "        from skimage.metrics import structural_similarity as sk_ssim\n",
    "        hr01 = (hr_f - hr_min_f)/(data_range_f + eps)\n",
    "        sr01 = (sr_f - hr_min_f)/(data_range_f + eps)\n",
    "        H = int(np.sqrt(hr01.size)) or 1; W = int(np.ceil(hr01.size / H))\n",
    "        pad = H*W - hr01.size\n",
    "        if pad>0:\n",
    "            hr01 = np.pad(hr01, (0,pad), constant_values=0)\n",
    "            sr01 = np.pad(sr01, (0,pad), constant_values=0)\n",
    "        ssim_val = float(sk_ssim(hr01.reshape(H,W), sr01.reshape(H,W), data_range=1.0))\n",
    "\n",
    "print(\"\\n===== Final Model Metrics (Full Image) =====\")\n",
    "print(f\"PSNR: {psnr_val:.4f} dB\")\n",
    "print(f\"SSIM: {ssim_val:.4f}\")\n",
    "print(f\"MAE : {mae_val:.6f}\")\n",
    "print(f\"MSE : {mse_val:.6f}\")\n",
    "print(f\"RMSE: {rmse_val:.6f}\")\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "import rasterio\n",
    "from rasterio.transform import Affine\n",
    "\n",
    "def _round_to_16_leq(x, default=256):\n",
    "    if x < 16: return 16\n",
    "    return max(16, int(x // 16) * 16)\n",
    "\n",
    "def _safe_remove(path):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def _cleanup_sidecars(path):\n",
    "    exts = [\"\", \".aux.xml\", \".ovr\", \".msk\", \".msk.ovr\", \".msk.aux.xml\"]\n",
    "    for e in exts:\n",
    "        _safe_remove(path + e)\n",
    "\n",
    "def save_geotiff_no_hole(out_path, arr, ref_path, scale_in_model=1):\n",
    "   \n",
    "    with rasterio.open(ref_path) as src:\n",
    "        crs = src.crs\n",
    "        t   = src.transform\n",
    "\n",
    "    H, W = int(arr.shape[0]), int(arr.shape[1])\n",
    "\n",
    "    \n",
    "    med = float(np.nanmedian(arr)) if (np.isnan(arr).any() or np.isinf(arr).any()) else float(np.median(arr))\n",
    "\n",
    "\n",
    "    profile = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": H,\n",
    "        \"width\": W,\n",
    "        \"count\": 1,\n",
    "        \"dtype\": \"float32\",\n",
    "        \"compress\": \"lzw\",\n",
    "        \"crs\": crs,\n",
    "        \"transform\": t if scale_in_model == 1 else Affine(t.a/scale_in_model, t.b, t.c,\n",
    "                                                          t.d, t.e/scale_in_model, t.f),\n",
    "        \"photometric\": \"MINISBLACK\",\n",
    "    }\n",
    "\n",
    "    \n",
    "    bx = _round_to_16_leq(min(256, W))\n",
    "    by = _round_to_16_leq(min(256, H))\n",
    "    try:\n",
    "        with rasterio.Env(GDAL_TIFF_INTERNAL_MASK=\"YES\", BIGTIFF=\"IF_SAFER\"):\n",
    "            with rasterio.open(out_path, \"w\", **profile, tiled=True, blockxsize=bx, blockysize=by) as dst:\n",
    "                dst.write(clean, 1)\n",
    "                dst.write_mask(valid_mask)\n",
    "        print(f\"GTiff 写出成功（tiled=True, block={bx}x{by}）。\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(\">>> 警告：tiled 写出失败，将清理残留并回退条带。错误：\", repr(e))\n",
    "        _cleanup_sidecars(out_path)\n",
    "\n",
    "    # 回退：非 tiled（条带），显式指定 rows per strip\n",
    "    rps = min(512, H)  # ROWS_PER_STRIP\n",
    "    try:\n",
    "        with rasterio.Env(GDAL_TIFF_INTERNAL_MASK=\"YES\", BIGTIFF=\"IF_SAFER\"):\n",
    "            with rasterio.open(out_path, \"w\", **profile, tiled=False, blockysize=rps) as dst:\n",
    "                dst.write(clean, 1)\n",
    "                dst.write_mask(valid_mask)\n",
    "        print(f\"GTiff 写出成功（tiled=False, rows_per_strip={rps}）。\")\n",
    "    except Exception as e2:\n",
    "        # 仍失败：彻底清理后，用最小参数无压缩兜底\n",
    "        print(\">>> 二次写出仍失败，尝试无压缩最小参数。错误：\", repr(e2))\n",
    "        _cleanup_sidecars(out_path)\n",
    "        minimal = {\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"height\": H,\n",
    "            \"width\": W,\n",
    "            \"count\": 1,\n",
    "            \"dtype\": \"float32\",\n",
    "            \"crs\": crs,\n",
    "            \"transform\": t if scale_in_model == 1 else Affine(t.a/scale_in_model, t.b, t.c,\n",
    "                                                              t.d, t.e/scale_in_model, t.f),\n",
    "        }\n",
    "        with rasterio.open(out_path, \"w\", **minimal) as dst:\n",
    "            dst.write(clean, 1)\n",
    "            dst.write_mask(valid_mask)\n",
    "\n",
    "\n",
    "    # 自检\n",
    "    with rasterio.open(out_path) as chk:\n",
    "        m = chk.read_masks(1)\n",
    "        meta = chk.profile\n",
    "\n",
    "# 保存\n",
    "print(\">>> 保存前检查：NaN/Inf =\",\n",
    "      int(np.isnan(sr_orig).sum()), int(np.isinf(sr_orig).sum()))\n",
    "with rasterio.open(hr_image_path) as _src_chk:\n",
    "    print(\"    ref(H,W) =\", _src_chk.height, _src_chk.width, \" pred(H,W) =\", sr_orig.shape)\n",
    "\n",
    "out_tif = 'JAG/rdn_reconstructed_4.tif'\n",
    "save_geotiff_no_hole(out_tif, sr_orig, ref_path=hr_image_path, scale_in_model=SCALE_IN_MODEL)\n",
    "print(f\">>> 推理完成，GeoTIFF 保存：{out_tif}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3cc3f-db21-4610-b3e7-7f5a1fb7a669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, time, warnings, gc\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.makedirs('results_x4', exist_ok=True)\n",
    "\n",
    "\n",
    "HR_TIF = 'JAG/磁异常.tif'                 \n",
    "LR_TIF = 'JAG/lr_map_simulated_4t.tif'     \n",
    "SCALE   = 4                                \n",
    "\n",
    "HR_PATCH_SIZE   = 64\n",
    "HR_PATCH_STRIDE = 32\n",
    "PATCH_LR   = max(1, int(round(HR_PATCH_SIZE   / SCALE)))   \n",
    "STRIDE_LR  = max(1, int(round(HR_PATCH_STRIDE / SCALE)))   \n",
    "NUM_PATCH_SHOW = 4\n",
    "\n",
    "\n",
    "NUM_EPOCHS      = 300\n",
    "\n",
    "VAL_RATIO       = 0.1\n",
    "VAL_MAX_SAMPLES = 512\n",
    "USE_COMPILE     = False\n",
    "EARLY_STOP      = 8\n",
    "MODEL_PATH_G    = 'JAG/best_esrgan_x4_gen.pth'\n",
    "\n",
    "def _read_first_band(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        arr = src.read(1).astype(np.float32)\n",
    "        prof = src.profile\n",
    "    return arr, prof\n",
    "\n",
    "def _reproject_match(src_arr, src_prof, dst_prof):\n",
    "    if src_prof.get('crs') is None or dst_prof.get('crs') is None:\n",
    "        raise ValueError(\"reproject 需要 src/dst 都有 CRS\")\n",
    "    dst = np.zeros((dst_prof['height'], dst_prof['width']), dtype=np.float32)\n",
    "    reproject(\n",
    "        source=src_arr,\n",
    "        destination=dst,\n",
    "        src_transform=src_prof['transform'],\n",
    "        src_crs=src_prof['crs'],\n",
    "        dst_transform=dst_prof['transform'],\n",
    "        dst_crs=dst_prof['crs'],\n",
    "        resampling=Resampling.bilinear\n",
    "    )\n",
    "    return dst\n",
    "\n",
    "def _ensure_ratio_scale(hr_arr, hr_prof, lr_arr, lr_prof, scale=SCALE):\n",
    "    \n",
    "    hr_crs = hr_prof.get('crs'); lr_crs = lr_prof.get('crs')\n",
    "    can_reproject = (hr_crs is not None) and (lr_crs is not None)\n",
    "\n",
    "    \n",
    "    if can_reproject and lr_prof['crs'] != hr_prof['crs']:\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            lr_prof['crs'], hr_prof['crs'], lr_prof['width'], lr_prof['height'],\n",
    "            *rasterio.transform.array_bounds(lr_prof['height'], lr_prof['width'], lr_prof['transform'])\n",
    "        )\n",
    "        tmp_prof = lr_prof.copy()\n",
    "        tmp_prof.update(crs=hr_prof['crs'], transform=transform, width=width, height=height)\n",
    "        lr_arr = _reproject_match(lr_arr, lr_prof, tmp_prof)\n",
    "        lr_prof = tmp_prof\n",
    "\n",
    "    \n",
    "    hr_a, hr_e = hr_prof['transform'].a, hr_prof['transform'].e\n",
    "    dst_transform = rasterio.Affine(\n",
    "        hr_a * scale, 0, hr_prof['transform'].c,\n",
    "        0,   hr_e * scale, hr_prof['transform'].f\n",
    "    )\n",
    "    dst_width  = int(np.ceil(hr_prof['width']  / scale))\n",
    "    dst_height = int(np.ceil(hr_prof['height'] / scale))\n",
    "    dst_prof = hr_prof.copy()\n",
    "    dst_prof.update(transform=dst_transform, width=dst_width, height=dst_height)\n",
    "\n",
    "    if can_reproject:\n",
    "        lr_arr = _reproject_match(lr_arr, lr_prof, dst_prof)\n",
    "        lr_prof = dst_prof\n",
    "    else:\n",
    "        print(\"跳过\")\n",
    "\n",
    "\n",
    "    H_lr = min(lr_arr.shape[0], hr_arr.shape[0] // scale)\n",
    "    W_lr = min(lr_arr.shape[1], hr_arr.shape[1] // scale)\n",
    "    lr_arr = lr_arr[:H_lr, :W_lr]\n",
    "    hr_arr = hr_arr[:H_lr*scale, :W_lr*scale]\n",
    "    return hr_arr, hr_prof, lr_arr, lr_prof\n",
    "\n",
    "def image_normalization(arr):\n",
    "    mn = np.nanmin(arr); mx = np.nanmax(arr)\n",
    "    rg = mx - mn\n",
    "    if rg < 1e-6: rg = 1.0\n",
    "    out = (arr - mn) / rg\n",
    "    out = np.nan_to_num(out, nan=0.0, posinf=1.0, neginf=0.0).astype(np.float32)\n",
    "    return out, float(mn), float(mx), float(rg)\n",
    "\n",
    "def extract_pairs(hr01, lr01, scale=SCALE, patch_lr=PATCH_LR, stride_lr=STRIDE_LR):\n",
    "\n",
    "    H_lr, W_lr = lr01.shape\n",
    "    ps, st = patch_lr, stride_lr\n",
    "    hr_patches, lr_patches = [], []\n",
    "    for i in range(0, H_lr - ps + 1, st):\n",
    "        for j in range(0, W_lr - ps + 1, st):\n",
    "            lr_patch = lr01[i:i+ps, j:j+ps]\n",
    "            hi, hj = i*scale, j*scale\n",
    "            hr_patch = hr01[hi:hi+ps*scale, hj:hj+ps*scale]\n",
    "            if hr_patch.shape == (ps*scale, ps*scale):\n",
    "                lr_patches.append(lr_patch.copy())\n",
    "                hr_patches.append(hr_patch.copy())\n",
    "    return np.asarray(hr_patches, np.float32), np.asarray(lr_patches, np.float32)\n",
    "\n",
    "def preprocess_all(hr_path, lr_path, scale, patch_lr, stride_lr, num_show=4):\n",
    "    t0 = time.time()\n",
    "    print(\">> 读取 HR/LR ...\")\n",
    "    hr_arr, hr_prof = _read_first_band(hr_path)\n",
    "    lr_arr, lr_prof = _read_first_band(lr_path)\n",
    "\n",
    "    print(f\">> 对齐网格（确保 LR:HR = 1:{scale}） ...\")\n",
    "    hr_arr, hr_prof, lr_arr, lr_prof = _ensure_ratio_scale(hr_arr, hr_prof, lr_arr, lr_prof, scale)\n",
    "\n",
    "    print(\">> 归一化（以 HR 的 min-max 为准） ...\")\n",
    "    hr01, hr_min, hr_max, hr_range = image_normalization(hr_arr)\n",
    "    lr01 = np.clip((lr_arr - hr_min) / (hr_range if hr_range != 0 else 1.0), 0, 1).astype(np.float32)\n",
    "\n",
    "    print(f\">> 切 patch：HR规格≈{HR_PATCH_SIZE} / {HR_PATCH_STRIDE}  →  LR规格={patch_lr} / {stride_lr}\")\n",
    "    hr_p, lr_p = extract_pairs(hr01, lr01, scale, patch_lr, stride_lr)\n",
    "    if hr_p.size == 0:\n",
    "        raise RuntimeError(\"未切出任何 patch，检查尺寸/stride/patch。\")\n",
    "    print(f\">> 样本数：{len(hr_p)}\")\n",
    "\n",
    "    # 预览\n",
    "    nshow = min(num_show, len(hr_p))\n",
    "    if nshow > 0:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        for k in range(nshow):\n",
    "            plt.subplot(2, nshow, k+1)\n",
    "            plt.imshow(lr_p[k], cmap='gray'); plt.axis('off'); plt.title(f'LR#{k+1}')\n",
    "            plt.subplot(2, nshow, k+1+nshow)\n",
    "            plt.imshow(hr_p[k], cmap='gray'); plt.axis('off'); plt.title(f'HR#{k+1}')\n",
    "        plt.tight_layout(); plt.savefig('results_x4/patch_examples_x4.png', dpi=200); plt.close()\n",
    "\n",
    "    print(f\">> 预处理完成，用时 {time.time()-t0:.1f}s\")\n",
    "    return (hr_p, lr_p, hr01, lr01, hr_min, hr_max, hr_range, hr_prof, lr_prof)\n",
    "\n",
    "class ResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, nf=64, gc=32):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(nf, gc, 3, 1, 1)\n",
    "        self.c2 = nn.Conv2d(nf+gc, gc, 3, 1, 1)\n",
    "        self.c3 = nn.Conv2d(nf+gc*2, gc, 3, 1, 1)\n",
    "        self.c4 = nn.Conv2d(nf+gc*3, gc, 3, 1, 1)\n",
    "        self.c5 = nn.Conv2d(nf+gc*4, nf, 3, 1, 1)\n",
    "        self.a  = nn.LeakyReLU(0.2, inplace=True)\n",
    "    def forward(self, x):\n",
    "        x1 = self.a(self.c1(x))\n",
    "        x2 = self.a(self.c2(torch.cat([x, x1], 1)))\n",
    "        x3 = self.a(self.c3(torch.cat([x, x1, x2], 1)))\n",
    "        x4 = self.a(self.c4(torch.cat([x, x1, x2, x3], 1)))\n",
    "        x5 = self.c5(torch.cat([x, x1, x2, x3, x4], 1))\n",
    "        return x + 0.2*x5\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "    def __init__(self, nf=64, gc=32):\n",
    "        super().__init__()\n",
    "        self.b1 = ResidualDenseBlock(nf, gc)\n",
    "        self.b2 = ResidualDenseBlock(nf, gc)\n",
    "        self.b3 = ResidualDenseBlock(nf, gc)\n",
    "    def forward(self, x):\n",
    "        y = self.b1(x); y = self.b2(y); y = self.b3(y)\n",
    "        return x + 0.2*y\n",
    "\n",
    "class ESRGANGenerator(nn.Module):\n",
    "    def __init__(self, in_nc=1, out_nc=1, nf=64, nb=8, gc=32, scale=SCALE):\n",
    "        super().__init__()\n",
    "        assert scale in (2,3,4,8), \"scale 仅支持 2/3/4/8\"\n",
    "        self.scale = scale\n",
    "        self.head = nn.Conv2d(in_nc, nf, 3, 1, 1)\n",
    "        self.trunk = nn.Sequential(*[RRDB(nf, gc) for _ in range(nb)])\n",
    "        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1)\n",
    "        self.up1 = nn.Conv2d(nf, nf, 3, 1, 1)\n",
    "        self.up2 = nn.Conv2d(nf, nf, 3, 1, 1)\n",
    "        self.up3 = nn.Conv2d(nf, nf, 3, 1, 1)\n",
    "        self.tail = nn.Conv2d(nf, out_nc, 3, 1, 1)\n",
    "        self.a = nn.LeakyReLU(0.2, inplace=True)\n",
    "    def forward(self, x):\n",
    "        fea = self.head(x)\n",
    "        trunk = self.trunk_conv(self.trunk(fea))\n",
    "        fea = fea + trunk\n",
    "        if self.scale == 2:\n",
    "            fea = self.a(self.up1(F.interpolate(fea, scale_factor=2, mode='nearest')))\n",
    "        elif self.scale == 3:\n",
    "            fea = self.a(self.up1(F.interpolate(fea, scale_factor=3, mode='nearest')))\n",
    "        elif self.scale == 4:\n",
    "            fea = self.a(self.up1(F.interpolate(fea, scale_factor=2, mode='nearest')))\n",
    "            fea = self.a(self.up2(F.interpolate(fea, scale_factor=2, mode='nearest')))\n",
    "        elif self.scale == 8:\n",
    "            fea = self.a(self.up1(F.interpolate(fea, scale_factor=2, mode='nearest')))\n",
    "            fea = self.a(self.up2(F.interpolate(fea, scale_factor=2, mode='nearest')))\n",
    "            fea = self.a(self.up3(F.interpolate(fea, scale_factor=2, mode='nearest')))\n",
    "        out = self.tail(fea)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PairSet(Dataset):\n",
    "    def __init__(self, hr_p, lr_p):\n",
    "        self.hr = np.ascontiguousarray(hr_p.astype(np.float32))  # (N, SCALE*P, SCALE*P)\n",
    "        self.lr = np.ascontiguousarray(lr_p.astype(np.float32))  # (N, P, P)\n",
    "    def __len__(self): return self.hr.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        lr = torch.from_numpy(self.lr[i]).unsqueeze(0)\n",
    "        hr = torch.from_numpy(self.hr[i]).unsqueeze(0)\n",
    "        return lr, hr\n",
    "\n",
    "use_torchmetrics = True\n",
    "try:\n",
    "    from torchmetrics.functional import structural_similarity_index_measure as tm_ssim\n",
    "    from torchmetrics.functional import peak_signal_noise_ratio as tm_psnr\n",
    "except Exception:\n",
    "    use_torchmetrics = False\n",
    "    from skimage.metrics import structural_similarity as sk_ssim\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_on_val(gen, loader, eval_max=512, device='cuda'):\n",
    "   \n",
    "    gen.eval()\n",
    "    n=0; psnrL=[]; ssimL=[]; maeL=[]; rmseL=[]\n",
    "    for lr, hr in loader:\n",
    "        if n>=eval_max: break\n",
    "        lr=lr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        hr=hr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            sr=gen(lr)\n",
    "        H=min(sr.shape[-2], hr.shape[-2]); W=min(sr.shape[-1], hr.shape[-1])\n",
    "        sr=sr[..., :H,:W].clamp_(0,1); hr=hr[..., :H,:W].clamp_(0,1)\n",
    "\n",
    "        if use_torchmetrics:\n",
    "            ps=tm_psnr(sr, hr, data_range=1.0)\n",
    "            ss=tm_ssim(sr, hr, data_range=1.0)\n",
    "            ma=torch.mean(torch.abs(sr-hr))\n",
    "            mse=torch.mean((sr-hr)**2)\n",
    "            rmse=torch.sqrt(mse)\n",
    "            psnrL.append(ps.detach().float().item())\n",
    "            ssimL.append(ss.detach().float().item())\n",
    "            maeL.append(ma.detach().float().item())\n",
    "            rmseL.append(rmse.detach().float().item())\n",
    "        else:\n",
    "            sr_np=sr.squeeze(1).detach().cpu().numpy()\n",
    "            hr_np=hr.squeeze(1).detach().cpu().numpy()\n",
    "            for i in range(sr_np.shape[0]):\n",
    "                diff=hr_np[i]-sr_np[i]\n",
    "                mse=float((diff*diff).mean()); rmse=float(np.sqrt(mse))\n",
    "                ps=20*np.log10(1.0/(np.sqrt(mse)+1e-12))\n",
    "                ss=sk_ssim(hr_np[i], sr_np[i], data_range=1.0)\n",
    "                ma=float(np.abs(diff).mean())\n",
    "                psnrL.append(float(ps)); ssimL.append(float(ss)); maeL.append(float(ma)); rmseL.append(float(rmse))\n",
    "        n+=lr.size(0)\n",
    "\n",
    "    if not psnrL: return 0.0,0.0,0.0,0.0\n",
    "    return float(np.mean(psnrL)), float(np.mean(ssimL)), float(np.mean(maeL)), float(np.mean(rmseL))\n",
    "\n",
    "\n",
    "def main():\n",
    "  \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "  \n",
    "    (hr_patches, lr_patches, hr01_full, lr01_full,\n",
    "     hr_min, hr_max, hr_range, hr_prof, lr_prof) = preprocess_all(\n",
    "        HR_TIF, LR_TIF, SCALE, PATCH_LR, STRIDE_LR, NUM_PATCH_SHOW\n",
    "    )\n",
    "\n",
    "    # 切分数据\n",
    "    N = len(hr_patches)\n",
    "    perm = np.random.permutation(N)\n",
    "    split = int(N*(1-VAL_RATIO))\n",
    "    train_idx, val_idx = perm[:split], perm[split:]\n",
    "    train_set = PairSet(hr_patches[train_idx], lr_patches[train_idx])\n",
    "    val_set   = PairSet(hr_patches[val_idx],  lr_patches[val_idx])\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    num_workers = min(8, os.cpu_count() or 4)\n",
    "    pin = torch.cuda.is_available()\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin, persistent_workers=(num_workers>0),\n",
    "                              prefetch_factor=4, drop_last=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=min(BATCH_SIZE,32), shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin, persistent_workers=(num_workers>0),\n",
    "\n",
    "    G = ESRGANGenerator(scale=SCALE, nb=8).to(device).to(memory_format=torch.channels_last)\n",
    "    if USE_COMPILE:\n",
    "        try: G=torch.compile(G, mode=\"reduce-overhead\")\n",
    "        except Exception: pass\n",
    "\n",
    "    opt = optim.AdamW(G.parameters(), lr=INIT_LR, fused=torch.cuda.is_available())\n",
    "    crit = nn.L1Loss()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "    # 调度器：以 score（越大越好）为依据\n",
    "    sched = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', factor=0.5, patience=3, verbose=True, min_lr=1e-6)\n",
    "\n",
    "    best_score=-1e9; epochs_no_improve=0\n",
    "    def _getlr(optim_):\n",
    "        for pg in optim_.param_groups: return pg.get(\"lr\", None)\n",
    "\n",
    "    try:\n",
    "run/max(1,nstep):.6f} time={time.time()-t0:.1f}s\")\n",
    "\n",
    "        for epoch in range(1, NUM_EPOCHS+1):\n",
    "            G.train(); t0=time.time(); run=0.0; nstep=0\n",
    "            for lr,hr in train_loader:\n",
    "                lr=lr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "                hr=hr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                    sr=G(lr); loss=crit(sr,hr)\n",
    "                scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
    "                run+=float(loss.item()); nstep+=1\n",
    "            avg_loss=run/max(1,nstep)\n",
    "            log=f\"[Epoch {epoch}/{NUM_EPOCHS}] lr={_getlr(opt):.2e} loss={avg_loss:.6f} time={time.time()-t0:.1f}s\"\n",
    "\n",
    "            if epoch % EVAL_EVERY==0:\n",
    "                psnr, ssim, mae, rmse = evaluate_on_val(G, val_loader, eval_max=VAL_MAX_SAMPLES, device=device)\n",
    "                \n",
    "                log += f\" | val_mae={mae:.4f} val_rmse={rmse:.4f} val_ssim={ssim:.4f} val_psnr={psnr:.3f}\"\n",
    "     \n",
    "                score = psnr + 10.0*ssim\n",
    "                sched.step(score)\n",
    "                if score > best_score + 1e-6:\n",
    "                    best_score = score; epochs_no_improve=0\n",
    "                    torch.save(G.state_dict(), MODEL_PATH_G)\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                if epochs_no_improve >= EARLY_STOP:\n",
    "                    print(log); print(\">>> 早停触发\"); break\n",
    "            print(log)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        ckpt = MODEL_PATH_G.replace('.pth', '_interrupt.pth')\n",
    "        torch.save(G.state_dict(), ckpt)\n",
    "        print(f\"\\n>>> 收到 KeyboardInterrupt，已保存中断权重到 {ckpt}\")\n",
    "\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH_G):\n",
    "        torch.save(G.state_dict(), MODEL_PATH_G)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_full(model, lr01, device='cuda'):\n",
    "        model.eval()\n",
    "        x=torch.from_numpy(lr01.astype(np.float32)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            y=model(x).squeeze(0).squeeze(0).clamp_(0,1)\n",
    "        return y.detach().cpu().numpy()\n",
    "\n",
    "    best = ESRGANGenerator(scale=SCALE, nb=8).to(device)\n",
    "    if USE_COMPILE:\n",
    "        try: best=torch.compile(best, mode=\"reduce-overhead\")\n",
    "        except Exception: pass\n",
    "    best.load_state_dict(torch.load(MODEL_PATH_G, map_location=device))\n",
    "    best.eval()\n",
    "\n",
    "    sr01 = predict_full(best, lr01_full, device=device)\n",
    "\n",
    "\n",
    "    sr_orig = (sr01 * hr_range) + hr_min\n",
    "    sr_orig = np.clip(sr_orig, hr_min, hr_max).astype(np.float32)\n",
    "    hr_orig = (hr01_full * hr_range) + hr_min\n",
    "\n",
    "\n",
    "    hr_f = np.nan_to_num(hr_orig, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float64)\n",
    "    sr_f = np.nan_to_num(sr_orig, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float64)\n",
    "    H = min(hr_f.shape[0], sr_f.shape[0]); W = min(hr_f.shape[1], sr_f.shape[1])\n",
    "    hr_f, sr_f = hr_f[:H,:W], sr_f[:H,:W]\n",
    "\n",
    "    diff = sr_f - hr_f\n",
    "    mse_val = float(np.mean(diff*diff))\n",
    "    mae_val = float(np.mean(np.abs(diff)))\n",
    "    rmse_val = float(np.sqrt(mse_val))\n",
    "    hr_min_f = float(np.min(hr_f)); hr_max_f = float(np.max(hr_f))\n",
    "    rg = hr_max_f - hr_min_f; eps=1e-12\n",
    "    psnr_val = (20*np.log10(rg/np.sqrt(mse_val+eps))) if rg>=eps else (float('inf') if mse_val<eps else 20*np.log10(1.0/np.sqrt(mse_val+eps)))\n",
    "\n",
    "    try:\n",
    "        from skimage.metrics import structural_similarity as sk_ssim\n",
    "        hr01m = (hr_f - hr_min_f)/(rg+eps); sr01m=(sr_f - hr_min_f)/(rg+eps)\n",
    "        ssim_val = float(sk_ssim(hr01m, sr01m, data_range=1.0))\n",
    "    except Exception:\n",
    "        ssim_val = float('nan')\n",
    "\n",
    "    print(\"\\n===== Final Model Metrics (Full Image) =====\")\n",
    "    print(f\"PSNR: {psnr_val:.4f} dB\")\n",
    "    print(f\"SSIM: {ssim_val:.4f}\")\n",
    "    print(f\"MAE : {mae_val:.6f}\")\n",
    "    print(f\"MSE : {mse_val:.6f}\")\n",
    "    print(f\"RMSE: {rmse_val:.6f}\")\n",
    "    print(\"============================================\\n\")\n",
    "    new_prof.update(\n",
    "        height=sr_orig.shape[0],\n",
    "        width =sr_orig.shape[1],\n",
    "        dtype='float32',\n",
    "        count=1,\n",
    "        transform=rasterio.Affine(\n",
    "            lr_prof['transform'].a/SCALE, 0, lr_prof['transform'].c,\n",
    "            0, lr_prof['transform'].e/SCALE, lr_prof['transform'].f\n",
    "        )\n",
    "    )\n",
    "    out_tif = 'JAG/esrgan_x4_reconstructed.tif'\n",
    "    with rasterio.open(out_tif, 'w', **new_prof) as dst:\n",
    "        dst.write(sr_orig, 1)\n",
    "    print(f\">>> 推理完成，GeoTIFF 保存：{out_tif}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc1daf-9985-4352-8a9e-358f3280b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SRFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98e23b-b320-4d5f-a7f4-c1fa0f2195d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "NUM_EPOCHS      = 1000\n",
    "INIT_LR         = 2e-4\n",
    "BATCH_SIZE      = 64                \n",
    "EVAL_EVERY      = 1                 \n",
    "DS_FACTOR       = 1                  \n",
    "EARLY_STOP      = 20                 \n",
    "USE_COMPILE     = True               \n",
    "MODEL_PATH      = 'JAG/best_srformer_fast.pth'\n",
    "\n",
    "W_SSIM = 0.7\n",
    "W_MAE  = 0.3\n",
    "\n",
    "#\n",
    "# ----------------------------\n",
    "data = np.load('JAG/preprocessed_data_4.npz', allow_pickle=True)\n",
    "\n",
    "hr_min  = float(data['hr_min'])\n",
    "hr_max  = float(data['hr_max'])\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "   \n",
    "    def __init__(self, drop_prob: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # (B,1,1,1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "class PermutedSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.h_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads,\n",
    "                                            dropout=attn_drop, batch_first=True)\n",
    "        self.w_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads,\n",
    "                                            dropout=attn_drop, batch_first=True)\n",
    "        self.proj = nn.Conv2d(dim, dim, kernel_size=1)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        x_h = x.permute(0, 3, 2, 1).reshape(B * W, H, C).contiguous()  # (B*W, H, C)\n",
    "        y_h, _ = self.h_attn(x_h, x_h, x_h, need_weights=False)\n",
    "        y_h = y_h.reshape(B, W, H, C).permute(0, 3, 2, 1).contiguous()  # (B, C, H, W)\n",
    "        \n",
    "        x_w = x.permute(0, 2, 3, 1).reshape(B * H, W, C).contiguous()  # (B*H, W, C)\n",
    "        y_w, _ = self.w_attn(x_w, x_w, x_w, need_weights=False)\n",
    "        y_w = y_w.reshape(B, H, W, C).permute(0, 3, 1, 2).contiguous()  # (B, C, H, W)\n",
    "        \n",
    "        y = 0.5 * (y_h + y_w)\n",
    "        y = self.proj(y)\n",
    "        y = self.proj_drop(y)\n",
    "        return y\n",
    "\n",
    "class ConvMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim, expansion=2.0, drop=0.0):\n",
    "        super().__init__()\n",
    "        hidden = int(dim * expansion)\n",
    "        self.pw1 = nn.Conv2d(dim, hidden, kernel_size=1, bias=True)\n",
    "        self.dw  = nn.Conv2d(hidden, hidden, kernel_size=3, padding=1, groups=hidden, bias=True)\n",
    "        self.act = nn.GELU()\n",
    "        self.pw2 = nn.Conv2d(hidden, dim, kernel_size=1, bias=True)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "    def forward(self, x):\n",
    "        x = self.pw1(x)\n",
    "        x = self.dw(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pw2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class LayerNorm2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_channels, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(num_channels, eps=eps)\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        x = x.permute(0, 2, 3, 1)                 # -> (B, H, W, C)\n",
    "        x = self.norm(x)\n",
    "        return x.permute(0, 3, 1, 2).contiguous() # -> (B, C, H, W)\n",
    "\n",
    "class SRFormerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, mlp_ratio=2.0, attn_drop=0.0, proj_drop=0.0, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = LayerNorm2d(dim)\n",
    "        self.attn  = PermutedSelfAttention(dim, num_heads=num_heads, attn_drop=attn_drop, proj_drop=proj_drop)\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = LayerNorm2d(dim)\n",
    "        self.mlp   = ConvMLP(dim, expansion=mlp_ratio, drop=proj_drop)\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class SRFormer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_ch=1,\n",
    "                 embed_dim=64,\n",
    "                 depth=8,\n",
    "                 num_heads=8,\n",
    "                 mlp_ratio=2.0,\n",
    "                 attn_drop=0.0,\n",
    "                 proj_drop=0.0,\n",
    "                 drop_path_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.in_proj  = nn.Conv2d(in_ch, embed_dim, kernel_size=3, padding=1)\n",
    "        dpr = torch.linspace(0, drop_path_rate, steps=depth).tolist() if depth > 1 else [drop_path_rate]\n",
    "        blocks = []\n",
    "        for i in range(depth):\n",
    "            blocks.append(\n",
    "                SRFormerBlock(\n",
    "                    dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    attn_drop=attn_drop,\n",
    "                    proj_drop=proj_drop,\n",
    "                    drop_path=dpr[i]\n",
    "                )\n",
    "            )\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.out_proj = nn.Conv2d(embed_dim, in_ch, kernel_size=3, padding=1)\n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        x = self.in_proj(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x + inp  \n",
    "\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, hr_patches, lr_patches):\n",
    "        self.hr = np.ascontiguousarray(hr_patches.astype(np.float32))\n",
    "        self.lr = np.ascontiguousarray(lr_patches.astype(np.float32))\n",
    "    def __len__(self): return self.hr.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        lr = torch.from_numpy(self.lr[idx]).unsqueeze(0)  # (1,H,W)\n",
    "        hr = torch.from_numpy(self.hr[idx]).unsqueeze(0)\n",
    "        return lr, hr\n",
    "\n",
    "N = len(hr_patches)\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "split = int(N * (1 - VAL_RATIO))\n",
    "train_idx, val_idx = idx[:split], idx[split:]\n",
    "train_set = SuperResolutionDataset(hr_patches[train_idx], lr_patches[train_idx])\n",
    "val_set   = SuperResolutionDataset(hr_patches[val_idx],  lr_patches[val_idx])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_workers = min(8, os.cpu_count() or 4)\n",
    "pin = torch.cuda.is_available()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin,\n",
    "    persistent_workers=(num_workers > 0),\n",
    "    prefetch_factor=4,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=min(BATCH_SIZE, 32),\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin,\n",
    "    persistent_workers=(num_workers > 0),\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "\n",
    "use_torchmetrics = True\n",
    "try:\n",
    "    from torchmetrics.functional import structural_similarity_index_measure as tm_ssim\n",
    "    from torchmetrics.functional import peak_signal_noise_ratio as tm_psnr\n",
    "except Exception:\n",
    "    use_torchmetrics = False\n",
    "    from skimage.metrics import structural_similarity as sk_ssim\n",
    "\n",
    "def _downsample_torch(x, factor=1):\n",
    "    if factor <= 1: return x\n",
    "    return torch.nn.functional.avg_pool2d(x, kernel_size=factor, stride=factor, ceil_mode=False)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_on_val(model, loader, eval_max=512, ds_factor=2, device='cuda'):\n",
    "    \n",
    "    model.eval()\n",
    "    n_seen = 0\n",
    "    psnr_list, ssim_list, mae_list, rmse_list = [], [], [], []\n",
    "    for lr, hr in loader:\n",
    "        if n_seen >= eval_max:\n",
    "            break\n",
    "        lr = lr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        hr = hr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            sr = model(lr)\n",
    "        sr_eval = _downsample_torch(sr, ds_factor)\n",
    "        hr_eval = _downsample_torch(hr, ds_factor)\n",
    "        if use_torchmetrics:\n",
    "            psnr = tm_psnr(sr_eval, hr_eval, data_range=1.0)\n",
    "            ssim = tm_ssim(sr_eval, hr_eval, data_range=1.0)\n",
    "            mae  = torch.mean(torch.abs(sr_eval - hr_eval))\n",
    "            mse  = torch.mean((sr_eval - hr_eval) ** 2)\n",
    "            rmse = torch.sqrt(mse)\n",
    "            psnr_list.append(psnr.detach().float().item())\n",
    "            ssim_list.append(ssim.detach().float().item())\n",
    "            mae_list.append(mae.detach().float().item())\n",
    "            rmse_list.append(rmse.detach().float().item())\n",
    "        else:\n",
    "            sr_cpu = sr_eval.squeeze(1).detach().cpu().numpy()  # (B,H,W)\n",
    "            hr_cpu = hr_eval.squeeze(1).detach().cpu().numpy()\n",
    "            for i in range(sr_cpu.shape[0]):\n",
    "                diff = hr_cpu[i] - sr_cpu[i]\n",
    "                mse  = float(np.mean(diff * diff))\n",
    "                rmse = float(np.sqrt(mse))\n",
    "                psnr = 20 * np.log10(1.0 / (np.sqrt(mse) + 1e-12))\n",
    "                ssim = sk_ssim(hr_cpu[i], sr_cpu[i], data_range=1.0)\n",
    "                mae  = float(np.mean(np.abs(diff)))\n",
    "                psnr_list.append(psnr); ssim_list.append(ssim); mae_list.append(mae); rmse_list.append(rmse)\n",
    "        n_seen += lr.size(0)\n",
    "    if len(psnr_list) == 0:\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "    return (float(np.mean(psnr_list)), float(np.mean(ssim_list)),\n",
    "            float(np.mean(mae_list)), float(np.mean(rmse_list)))\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = SRFormer(in_ch=1, embed_dim=64, depth=8, num_heads=8,\n",
    "                 mlp_ratio=2.0, attn_drop=0.0, proj_drop=0.0,\n",
    "                 drop_path_rate=0.0).to(device).to(memory_format=torch.channels_last)\n",
    "if USE_COMPILE:\n",
    "    try:\n",
    "        model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=INIT_LR, fused=torch.cuda.is_available())\n",
    "criterion = nn.L1Loss()\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',       \n",
    "    factor=0.5,        \n",
    "    patience=10,      \n",
    "    verbose=True,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "best_score = -float('inf')  \n",
    "epochs_no_improve = 0\n",
    "\n",
    "def _get_lr(optim_):\n",
    "    for pg in optim_.param_groups:\n",
    "        return pg.get(\"lr\", None)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    running = 0.0\n",
    "    for lr, hr in train_loader:\n",
    "        lr = lr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        hr = hr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            pred = model(lr)\n",
    "            loss = criterion(pred, hr)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running += loss.item()\n",
    "    avg_loss = running / len(train_loader)\n",
    "    log = f\"[Epoch {epoch}/{NUM_EPOCHS}] lr={_get_lr(optimizer):.2e} loss={avg_loss:.6f} time={time.time()-t0:.1f}s\"\n",
    "    \n",
    "    if epoch % EVAL_EVERY == 0:\n",
    "        psnr, ssim, mae, rmse = evaluate_on_val(\n",
    "            model, val_loader,\n",
    "            eval_max=VAL_MAX_SAMPLES,\n",
    "            ds_factor=DS_FACTOR,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        score = W_SSIM * ssim + W_MAE * (1.0 - mae)\n",
    "        log += f\" | val_mae={mae:.4f} val_rmse={rmse:.4f} val_ssim={ssim:.4f} score={score:.4f}\"\n",
    "        \n",
    "        scheduler.step(score)\n",
    "       \n",
    "        if score > best_score + 1e-6:\n",
    "            best_score = score\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= EARLY_STOP:\n",
    "            print(log)\n",
    "            print(\">>> 早停触发（组合分数连续无改善）\")\n",
    "            break\n",
    "    print(log)\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_full(model, lr_image_01, device='cuda'):\n",
    "    model.eval()\n",
    "    x = torch.from_numpy(lr_image_01.astype(np.float32)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "        y = model(x).squeeze(0).squeeze(0).clamp_(0, 1)\n",
    "    return y.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "best = SRFormer(in_ch=1, embed_dim=64, depth=8, num_heads=8,\n",
    "                mlp_ratio=2.0, attn_drop=0.0, proj_drop=0.0,\n",
    "                drop_path_rate=0.0).to(device)\n",
    "if USE_COMPILE:\n",
    "    try:\n",
    "        best = torch.compile(best, mode=\"reduce-overhead\")\n",
    "    except Exception:\n",
    "        pass\n",
    "best.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "best.eval()\n",
    "\n",
    "sr_01 = predict_full(best, lr_array_norm, device=device)\n",
    "\n",
    "sr_orig = (sr_01 * hr_range) + hr_min\n",
    "sr_orig = np.clip(sr_orig, hr_min, hr_max).astype(np.float32)\n",
    "hr_orig = (hr_array_norm * hr_range) + hr_min\n",
    "\n",
    "hr_clean = np.nan_to_num(hr_orig, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float64)\n",
    "sr_clean = np.nan_to_num(sr_orig, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float64)\n",
    "\n",
    "\n",
    "finite_mask = np.isfinite(hr_clean) & np.isfinite(sr_clean)\n",
    "if finite_mask.sum() == 0:\n",
    "    raise RuntimeError(\"整图没有可用的有限像素\")\n",
    "\n",
    "hr_f = hr_clean[finite_mask]\n",
    "sr_f = sr_clean[finite_mask]\n",
    "\n",
    "hr_min_f = float(np.min(hr_f))\n",
    "hr_max_f = float(np.max(hr_f))\n",
    "data_range_f = hr_max_f - hr_min_f\n",
    "\n",
    "diff = sr_f - hr_f\n",
    "mse_val = float(np.mean(diff * diff))\n",
    "mae_val = float(np.mean(np.abs(diff)))\n",
    "rmse_val = float(np.sqrt(mse_val))\n",
    "\n",
    "eps = 1e-12\n",
    "if data_range_f < eps:\n",
    "    psnr_val = float('inf') if mse_val < eps else 20 * np.log10((1.0) / np.sqrt(mse_val + eps))\n",
    "else:\n",
    "    psnr_val = 20 * np.log10(data_range_f / np.sqrt(mse_val + eps))\n",
    "\n",
    "if data_range_f < eps:\n",
    "    ssim_val = 1.0 if mse_val < eps else 0.0\n",
    "else:\n",
    "    hr01 = (hr_f - hr_min_f) / (data_range_f + eps)\n",
    "    sr01 = (sr_f - hr_min_f) / (data_range_f + eps)\n",
    "    if use_torchmetrics and torch.cuda.is_available():\n",
    "        hr_t = torch.from_numpy(hr01.reshape(1, 1, -1, 1)).to(device)  # 形状不重要，只要是 2D\n",
    "        sr_t = torch.from_numpy(sr01.reshape(1, 1, -1, 1)).to(device)\n",
    "        with torch.amp.autocast('cuda', enabled=True):\n",
    "            from torchmetrics.functional import structural_similarity_index_measure as tm_ssim\n",
    "            ssim_val = float(tm_ssim(sr_t, hr_t, data_range=1.0).item())\n",
    "    else:\n",
    "        from skimage.metrics import structural_similarity as sk_ssim\n",
    "        H = int(np.sqrt(hr01.size)) or 1\n",
    "        W = int(np.ceil(hr01.size / H))\n",
    "        pad = H * W - hr01.size\n",
    "        if pad > 0:\n",
    "            hr01 = np.pad(hr01, (0, pad), constant_values=0)\n",
    "            sr01 = np.pad(sr01, (0, pad), constant_values=0)\n",
    "        hr_img = hr01.reshape(H, W)\n",
    "        sr_img = sr01.reshape(H, W)\n",
    "        ssim_val = float(sk_ssim(hr_img, sr_img, data_range=1.0))\n",
    "\n",
    "print(\"\\n===== Final Model Metrics (Full Image) =====\")\n",
    "print(f\"PSNR: {psnr_val:.4f} dB\")\n",
    "print(f\"SSIM: {ssim_val:.4f}\")\n",
    "print(f\"MAE : {mae_val:.6f}\")\n",
    "print(f\"MSE : {mse_val:.6f}\")\n",
    "print(f\"RMSE: {rmse_val:.6f}\")\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "# ====== 保存 GeoTIFF ======\n",
    "import rasterio\n",
    "with rasterio.open(hr_image_path) as src:\n",
    "    profile = src.profile\n",
    "profile.update(count=1, dtype='float32')\n",
    "\n",
    "out_tif = 'JAG/srformer_reconstructed_4.tif'\n",
    "with rasterio.open(out_tif, 'w', **profile) as dst:\n",
    "    dst.write(sr_orig, 1)\n",
    "\n",
    "print(f\">>> 推理完成，GeoTIFF 保存：{out_tif}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8fd804-d328-4cd4-a116-f23993414218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####FGSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c1daa-7909-46fd-b7d9-cbfe995e39d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.makedirs('JAG', exist_ok=True)\n",
    "\n",
    "NUM_EPOCHS      = 1000\n",
    "INIT_LR         = 1e-4\n",
    "BATCH_SIZE      = 32\n",
    "EVAL_EVERY      = 1         \n",
    "VAL_RATIO       = 0.1\n",
    "VAL_MAX_SAMPLES = 512\n",
    "DS_FACTOR       = 4\n",
    "EARLY_STOP      = 6\n",
    "USE_COMPILE     = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MODEL_PATH      = 'JAG/best_srcnn_fma_small_anom_aggressive.pth'\n",
    "\n",
    "hr_image_path   = 'JAG/磁异常.tif'\n",
    "\n",
    "\n",
    "data = np.load('JAG/preprocessed_data_4.npz', allow_pickle=True)\n",
    "hr_patches    = data['hr_patches']         # (N, H, W), 0~1\n",
    "lr_patches    = data['lr_patches']         # (N, H, W), 0~1\n",
    "hr_array_norm = data['hr_array_norm']      # (H, W),   0~1\n",
    "lr_array_norm = data['lr_array_norm']      # (H, W),   0~1\n",
    "hr_min  = float(data['hr_min'])\n",
    "hr_max  = float(data['hr_max'])\n",
    "hr_range = float(data['hr_range'])\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    B, C, H, W = x.shape\n",
    "    pad_h = (window_size - H % window_size) % window_size\n",
    "    pad_w = (window_size - W % window_size) % window_size\n",
    "    if pad_h or pad_w:\n",
    "        x = F.pad(x, (0, pad_w, 0, pad_h), mode=\"reflect\")\n",
    "    Hp, Wp = x.shape[-2:]\n",
    "    x = x.view(B, C, Hp // window_size, window_size, Wp // window_size, window_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5).contiguous().view(-1, C, window_size, window_size)\n",
    "    return x, Hp, Wp, (pad_h, pad_w)\n",
    "\n",
    "def window_unpartition(windows, Hp, Wp, window_size, pad):\n",
    "    pad_h, pad_w = pad\n",
    "    B_ = windows.shape[0] // ((Hp // window_size) * (Wp // window_size))\n",
    "    C = windows.shape[1]\n",
    "    x = windows.view(B_, Hp // window_size, Wp // window_size, C, window_size, window_size)\n",
    "    x = x.permute(0, 3, 1, 4, 2, 5).contiguous().view(B_, C, Hp, Wp)\n",
    "    if pad_h or pad_w:\n",
    "        x = x[:, :, :Hp - pad_h, :Wp - pad_w]\n",
    "    return x\n",
    "\n",
    "class LayerNormCF(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        return self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "\n",
    "class FourierUnitEnhanced(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, groups=1, fft_norm='ortho', bottleneck_ratio=0.75):\n",
    "        super().__init__()\n",
    "        self.groups = groups\n",
    "        self.fft_norm = fft_norm\n",
    "        mid = max(1, int(dim * bottleneck_ratio))\n",
    "        self.proj_in  = nn.Conv2d(dim * 2, mid * 2, 1, bias=False, groups=self.groups)\n",
    "        self.act      = nn.GELU()\n",
    "        self.proj_out = nn.Conv2d(mid * 2, dim * 2, 1, bias=False, groups=self.groups)\n",
    "        self.mag_gate = nn.Sequential(\n",
    "            nn.Conv2d(dim, max(1, dim // 2), 1, bias=True),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(max(1, dim // 2), dim, 1, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        with torch.amp.autocast('cuda', enabled=False):\n",
    "            x32 = x.float()\n",
    "            Xf = torch.fft.rfft2(x32, norm=self.fft_norm)   # complex64\n",
    "            mag = torch.abs(Xf).float()\n",
    "            gate = self.mag_gate(mag)\n",
    "            Xf = Xf * gate\n",
    "            real = Xf.real\n",
    "            imag = Xf.imag\n",
    "            cat = torch.cat([real, imag], dim=1).float()\n",
    "            y = self.proj_in(cat); y = self.act(y); y = self.proj_out(y)\n",
    "            real2, imag2 = torch.chunk(y, 2, dim=1)\n",
    "            Xf2 = torch.complex(real2, imag2)\n",
    "            out32 = torch.fft.irfft2(Xf2, s=(H, W), norm=self.fft_norm).float()\n",
    "        return out32.to(x.dtype)\n",
    "\n",
    "class FMAPlus(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=64, num_heads=16, window_size=3, bottleneck_ratio=0.75, temp_init=3.0):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, \n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.norm = LayerNormCF(dim, eps=1e-6)\n",
    "        self.fourier = FourierUnitEnhanced(dim, bottleneck_ratio=bottleneck_ratio)\n",
    "\n",
    "        self.v_proj1 = nn.Conv2d(dim, dim, kernel_size=1, bias=False)\n",
    "        self.v_dw    = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim, bias=False)\n",
    "        self.v_act   = nn.GELU()\n",
    "        self.v_proj2 = nn.Conv2d(dim, dim, kernel_size=1, bias=False)\n",
    "\n",
    "        self._temp = nn.Parameter(torch.ones(num_heads) * float(temp_init))\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "        self.cpe  = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim)\n",
    "        self.proj = nn.Conv2d(dim, dim, kernel_size=1, bias=True)\n",
    "        self.layer_scale = nn.Parameter(1e-6 * torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        shortcut = x\n",
    "        pos = self.cpe(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        A = self.fourier(x)\n",
    "        V = self.v_proj1(x); V = self.v_dw(V); V = self.v_act(V); V = self.v_proj2(V)\n",
    "\n",
    "        w = self.window_size\n",
    "        A_win, Hp, Wp, pad = window_partition(A, w)\n",
    "        V_win, _,  _,  _   = window_partition(V, w)\n",
    "\n",
    "        A_tok = rearrange(A_win, 'bn (h c) a b -> bn h c (a b)', h=self.num_heads)\n",
    "        V_tok = rearrange(V_win, 'bn (h c) a b -> bn h c (a b)', h=self.num_heads)\n",
    "\n",
    "        temp = self.softplus(self._temp).view(1, self.num_heads, 1, 1)\n",
    "        attn = (A_tok * V_tok) * temp\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        Xw = rearrange(attn, 'bn h c (a b) -> bn (h c) a b', a=w, b=w)\n",
    "        X  = window_unpartition(Xw, Hp, Wp, w, pad)\n",
    "\n",
    "        X  = X + pos\n",
    "        X  = self.proj(X)\n",
    "        X  = self.layer_scale.view(1, -1, 1, 1) * X\n",
    "        return X + shortcut\n",
    "\n",
    "class MultiScaleFMA(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=64, heads=16, win_small=3, win_mid=5, bottle=0.75, temp_init=3.0):\n",
    "        super().__init__()\n",
    "        self.fma_s = FMAPlus(dim, heads, win_small, bottle, temp_init)\n",
    "        self.fma_m = FMAPlus(dim, heads, win_mid,   bottle, temp_init)\n",
    "        self.gate  = nn.Parameter(torch.tensor(0.5))\n",
    "    def forward(self, x):\n",
    "        xs = self.fma_s(x)\n",
    "        xm = self.fma_m(x)\n",
    "        g  = torch.sigmoid(self.gate)\n",
    "        return g*xs + (1-g)*xm\n",
    "\n",
    "class HighPassResidual(nn.Module):\n",
    "\n",
    "    def __init__(self, ch=64):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv2d(ch, ch, 3, 1, 1, groups=ch, bias=False)\n",
    "        k = torch.tensor([[0., -1.,  0.],\n",
    "                          [-1., 4., -1.],\n",
    "                          [0., -1.,  0.]]).view(1,1,3,3)\n",
    "        self.register_buffer('lap', k)\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5))\n",
    "    def forward(self, x):\n",
    "        y1 = self.dw(x)\n",
    "        y2 = F.conv2d(x, self.lap.to(dtype=x.dtype, device=x.device).expand(x.size(1),1,3,3),\n",
    "                      padding=1, groups=x.size(1))\n",
    "        a = torch.sigmoid(self.alpha)\n",
    "        return x + a*y1 + (1-a)*y2\n",
    "class SRCNN_FMA_AnomAgg(nn.Module):\n",
    "    def __init__(self, dim=64, heads=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, dim, kernel_size=9, padding=4)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.hires = HighPassResidual(ch=dim)\n",
    "        self.ms1 = MultiScaleFMA(dim=dim, heads=heads, win_small=3, win_mid=5, bottle=0.75, temp_init=3.0)\n",
    "        self.ms2 = MultiScaleFMA(dim=dim, heads=heads, win_small=3, win_mid=5, bottle=0.75, temp_init=3.0)\n",
    "        self.ms3 = MultiScaleFMA(dim=dim, heads=heads, win_small=3, win_mid=5, bottle=0.75, temp_init=3.0)\n",
    "        self.conv2 = nn.Conv2d(dim, 32, kernel_size=1)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(32, 1, kernel_size=5, padding=2)\n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.hires(x)\n",
    "        x = self.ms1(x)\n",
    "        x = self.ms2(x)\n",
    "        x = self.ms3(x)\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x + inp\n",
    "\n",
    "class SRDataset(Dataset):\n",
    "    def __init__(self, hr_patches, lr_patches):\n",
    "        self.hr = np.ascontiguousarray(hr_patches.astype(np.float32))\n",
    "        self.lr = np.ascontiguousarray(lr_patches.astype(np.float32))\n",
    "    def __len__(self): return self.hr.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        lr = torch.from_numpy(self.lr[idx]).unsqueeze(0)\n",
    "        hr = torch.from_numpy(self.hr[idx]).unsqueeze(0)\n",
    "        return lr, hr\n",
    "\n",
    "def patch_variance_weights(arr):\n",
    "    v = np.var(arr.reshape(arr.shape[0], -1), axis=1) + 1e-6\n",
    "    w = v / v.mean()\n",
    "    w = np.clip(w, 0.2, 5.0)\n",
    "    return w.astype(np.float64)\n",
    "\n",
    "N = len(hr_patches)\n",
    "idx_all = np.arange(N); np.random.shuffle(idx_all)\n",
    "split = int(N * (1 - VAL_RATIO))\n",
    "train_idx, val_idx = idx_all[:split], idx_all[split:]\n",
    "\n",
    "train_set = SRDataset(hr_patches[train_idx], lr_patches[train_idx])\n",
    "val_set   = SRDataset(hr_patches[val_idx],  lr_patches[val_idx])\n",
    "\n",
    "weights = patch_variance_weights(hr_patches[train_idx])\n",
    "sampler = WeightedRandomSampler(weights=weights, num_samples=len(train_idx), replacement=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_workers = min(8, os.cpu_count() or 4)\n",
    "pin = torch.cuda.is_available()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin,\n",
    "    persistent_workers=(num_workers > 0),\n",
    "    prefetch_factor=4,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=min(BATCH_SIZE, 32),\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin,\n",
    "    persistent_workers=(num_workers > 0),\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "\n",
    "use_torchmetrics = True\n",
    "try:\n",
    "    from torchmetrics.functional import structural_similarity_index_measure as tm_ssim\n",
    "    from torchmetrics.functional import peak_signal_noise_ratio as tm_psnr\n",
    "except Exception:\n",
    "    use_torchmetrics = False\n",
    "    from skimage.metrics import structural_similarity as sk_ssim\n",
    "\n",
    "def _downsample_torch(x, factor=1):\n",
    "    if factor <= 1: return x\n",
    "    return F.avg_pool2d(x, kernel_size=factor, stride=factor, ceil_mode=False)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_on_val(model, loader, eval_max=512, ds_factor=2, device='cuda'):\n",
    "    model.eval()\n",
    "    n_seen = 0\n",
    "    psnr_list, ssim_list, mae_list, rmse_list = [], [], [], []\n",
    "    for lr, hr in loader:\n",
    "        if n_seen >= eval_max: break\n",
    "        lr = lr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        hr = hr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
    "            sr = model(lr)\n",
    "        H = min(sr.shape[-2], hr.shape[-2]); W = min(sr.shape[-1], hr.shape[-1])\n",
    "        sr = sr[..., :H, :W].clamp_(0, 1); hr = hr[..., :H, :W].clamp_(0, 1)\n",
    "        sr_eval = _downsample_torch(sr, ds_factor)\n",
    "        hr_eval = _downsample_torch(hr, ds_factor)\n",
    "        if use_torchmetrics:\n",
    "            psnr = tm_psnr(sr_eval, hr_eval, data_range=1.0)\n",
    "            ssim = tm_ssim(sr_eval, hr_eval, data_range=1.0)\n",
    "            mae  = torch.mean(torch.abs(sr_eval - hr_eval))\n",
    "            mse  = torch.mean((sr_eval - hr_eval) ** 2)\n",
    "            rmse = torch.sqrt(mse)\n",
    "            psnr_list.append(psnr.detach().float().item())\n",
    "            ssim_list.append(ssim.detach().float().item())\n",
    "            mae_list.append(mae.detach().float().item())\n",
    "            rmse_list.append(rmse.detach().float().item())\n",
    "        else:\n",
    "            sr_cpu = sr_eval.squeeze(1).detach().cpu().numpy()\n",
    "            hr_cpu = hr_eval.squeeze(1).detach().cpu().numpy()\n",
    "            for i in range(sr_cpu.shape[0]):\n",
    "                diff = hr_cpu[i] - sr_cpu[i]\n",
    "                mse  = float(np.mean(diff * diff))\n",
    "                rmse = float(np.sqrt(mse))\n",
    "                psnr = 20 * np.log10(1.0 / (np.sqrt(mse) + 1e-12))\n",
    "                ssim = sk_ssim(hr_cpu[i], sr_cpu[i], data_range=1.0)\n",
    "                mae  = float(np.mean(np.abs(diff)))\n",
    "                psnr_list.append(psnr); ssim_list.append(ssim); mae_list.append(mae); rmse_list.append(rmse)\n",
    "        n_seen += lr.size(0)\n",
    "    if not psnr_list: return 0.0, 0.0, 0.0, 0.0\n",
    "    return float(np.mean(psnr_list)), float(np.mean(ssim_list)), float(np.mean(mae_list)), float(np.mean(rmse_list))\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = SRCNN_FMA_AnomAgg(dim=64, heads=16).to(device).to(memory_format=torch.channels_last)\n",
    "if USE_COMPILE:\n",
    "    try:\n",
    "        model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=INIT_LR, fused=torch.cuda.is_available())\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=torch.cuda.is_available())\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=3, verbose=True, min_lr=1e-6\n",
    ")\n",
    "\n",
    "best_score = -1e9\n",
    "epochs_no_improve = 0\n",
    "\n",
    "def _get_lr(optim_):\n",
    "    for pg in optim_.param_groups:\n",
    "        return pg.get(\"lr\", None)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    running = 0.0\n",
    "    for lr, hr in train_loader:\n",
    "        lr = lr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        hr = hr.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
    "            pred = model(lr)\n",
    "            loss = criterion(pred.clamp(0,1), hr)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running += loss.item()\n",
    "\n",
    "    avg_loss = running / max(1, len(train_loader))\n",
    "    log = f\"[Epoch {epoch}/{NUM_EPOCHS}] lr={_get_lr(optimizer):.2e} loss={avg_loss:.6f} time={time.time()-t0:.1f}s\"\n",
    "\n",
    "    if epoch % EVAL_EVERY == 0:\n",
    "        psnr, ssim, mae, rmse = evaluate_on_val(model, val_loader, eval_max=VAL_MAX_SAMPLES, ds_factor=DS_FACTOR, device=device)\n",
    "        score = W_SSIM * ssim + W_MAE * (1.0 - mae)\n",
    "        log += f\" | val_mae={mae:.4f} val_rmse={rmse:.4f} val_ssim={ssim:.4f} score={score:.4f}\"\n",
    "\n",
    "        scheduler.step(score)\n",
    "\n",
    "        if score > best_score + 1e-6:\n",
    "            best_score = score\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= EARLY_STOP:\n",
    "            print(log); print(\">>> 早停触发（组合分数连续无改善）\"); break\n",
    "\n",
    "    print(log)\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_full(model, lr_image_01, device='cuda'):\n",
    "    model.eval()\n",
    "    x = torch.from_numpy(lr_image_01.astype(np.float32)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
    "        y = model(x).squeeze(0).squeeze(0).clamp_(0, 1)\n",
    "    return y.detach().cpu().numpy()\n",
    "\n",
    "best = SRCNN_FMA_AnomAgg(dim=64, heads=16).to(device)\n",
    "state = torch.load(MODEL_PATH, map_location=device)\n",
    "best.load_state_dict(state, strict=True)\n",
    "best.eval()\n",
    "\n",
    "sr_01 = predict_full(best, lr_array_norm, device=device)\n",
    "\n",
    "\n",
    "sr_orig = (sr_01 * hr_range) + hr_min\n",
    "sr_orig = np.clip(sr_orig, hr_min, hr_max).astype(np.float32)\n",
    "hr_orig = (hr_array_norm * hr_range) + hr_min\n",
    "\n",
    "hr_clean = np.nan_to_num(hr_orig, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float64)\n",
    "sr_clean = np.nan_to_num(sr_orig, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float64)\n",
    "finite_mask = np.isfinite(hr_clean) & np.isfinite(sr_clean)\n",
    "if finite_mask.sum() == 0:\n",
    "    raise RuntimeError()\n",
    "hr_f = hr_clean[finite_mask]; sr_f = sr_clean[finite_mask]\n",
    "hr_min_f = float(np.min(hr_f)); hr_max_f = float(np.max(hr_f))\n",
    "data_range_f = hr_max_f - hr_min_f\n",
    "diff = sr_f - hr_f\n",
    "mse_val = float(np.mean(diff * diff)); mae_val = float(np.mean(np.abs(diff))); rmse_val = float(np.sqrt(mse_val))\n",
    "\n",
    "eps = 1e-12\n",
    "if data_range_f < eps:\n",
    "    psnr_val = float('inf') if mse_val < eps else 20 * np.log10((1.0) / np.sqrt(mse_val + eps))\n",
    "else:\n",
    "    psnr_val = 20 * np.log10(data_range_f / np.sqrt(mse_val + eps))\n",
    "\n",
    "if data_range_f < eps:\n",
    "    ssim_val = 1.0 if mse_val < eps else 0.0\n",
    "else:\n",
    "    from skimage.metrics import structural_similarity as sk_ssim\n",
    "    hr01 = (hr_f - hr_min_f) / (data_range_f + eps)\n",
    "    sr01 = (sr_f - hr_min_f) / (data_range_f + eps)\n",
    "    H = int(np.sqrt(hr01.size)) or 1; W = int(np.ceil(hr01.size / H))\n",
    "    pad = H * W - hr01.size\n",
    "    if pad > 0:\n",
    "        hr01 = np.pad(hr01, (0, pad), constant_values=0)\n",
    "        sr01 = np.pad(sr01, (0, pad), constant_values=0)\n",
    "    hr_img = hr01.reshape(H, W); sr_img = sr01.reshape(H, W)\n",
    "    ssim_val = float(sk_ssim(hr_img, sr_img, data_range=1.0))\n",
    "\n",
    "print(\"\\n===== Final Model Metrics (Full Image) =====\")\n",
    "print(f\"PSNR: {psnr_val:.4f} dB\")\n",
    "print(f\"SSIM: {ssim_val:.4f}\")\n",
    "print(f\"MAE : {mae_val:.6f}\")\n",
    "print(f\"MSE : {mse_val:.6f}\")\n",
    "print(f\"RMSE: {rmse_val:.6f}\")\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "import rasterio\n",
    "with rasterio.open(hr_image_path) as src:\n",
    "    profile = src.profile\n",
    "profile.update(count=1, dtype='float32')\n",
    "\n",
    "out_tif = 'JAG/fgsa_reconstructed4.tif'\n",
    "with rasterio.open(out_tif, 'w', **profile) as dst:\n",
    "    dst.write(sr_orig, 1)\n",
    "\n",
    "print(f\">>> 推理完成，GeoTIFF 保存：{out_tif}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911996a-a007-467b-a99c-b201104e3f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
